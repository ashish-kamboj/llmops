{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c9b33c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an Ollama account at [ollama.com](https://ollama.com)\n",
    "2. Generate an API key from [ollama.com/settings/keys](https://ollama.com/settings/keys)\n",
    "3. Set the API key in the `.env` file in this folder\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install ollama python-dotenv requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2453f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing python-dotenv...\n",
      "‚úì All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages and import libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['ollama', 'python-dotenv', 'requests']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"‚úì All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e734ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Ollama API configured\n",
      "  Endpoint: https://ollama.com\n",
      "  Default Model: gpt-oss:120b\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use Path.cwd() since __file__ is not defined in Jupyter notebooks\n",
    "env_path = Path.cwd() / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    # Try to find .env in the ollama-cloud directory\n",
    "    load_dotenv(dotenv_path='d:\\\\ollama-n8n\\\\ollama-cloud\\\\.env')\n",
    "\n",
    "# Get API key and configuration\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "OLLAMA_ENDPOINT = os.getenv('OLLAMA_API_ENDPOINT', 'https://ollama.com')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'gpt-oss:120b')\n",
    "\n",
    "# Verify API key is set\n",
    "if not OLLAMA_API_KEY or OLLAMA_API_KEY == 'your_api_key_here':\n",
    "    print(\"‚ö†Ô∏è  WARNING: OLLAMA_API_KEY not set in .env file!\")\n",
    "    print(\"Please set your API key in the .env file: https://ollama.com/settings/keys\")\n",
    "else:\n",
    "    print(f\"‚úì Ollama API configured\")\n",
    "    print(f\"  Endpoint: {OLLAMA_ENDPOINT}\")\n",
    "    print(f\"  Default Model: {DEFAULT_MODEL}\")\n",
    "\n",
    "# Initialize Ollama client for cloud API\n",
    "client = ollama.Client(\n",
    "    host=OLLAMA_ENDPOINT,\n",
    "    headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba69f6",
   "metadata": {},
   "source": [
    "## Chat Capability - Basic Conversation\n",
    "\n",
    "The simplest use case: send a message and get a response. This works with any Ollama Cloud model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44034899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Sending prompt to gpt-oss:120b...\n",
      "   Prompt: What are the top 3 advantages of using cloud models?\n",
      "\n",
      "‚úì Response received:\n",
      "**Top‚ÄØ3 Advantages of Using Cloud‚ÄëBased Models**\n",
      "\n",
      "| # | Advantage | Why It Matters (Key Benefits) |\n",
      "|---|------------|------------------------------|\n",
      "| **1** | **Scalable, On‚ÄëDemand Resources** | ‚Ä¢ **Elasticity:** Compute, storage, and networking can be spun up or down in seconds to match the workload (e.g., spikes in inference traffic).<br>‚Ä¢ **Pay‚Äëas‚Äëyou‚Äëgo:** You‚Äôre billed only for what you actually use, eliminating the need for costly over‚Äëprovisioned hardware.<br>‚Ä¢ **Global Reach:** Multi‚Äëregion deployments let you place models close to end‚Äëusers, reducing latency and improving user experience. |\n",
      "| **2** | **Accelerated Development & Deployment** | ‚Ä¢ **Managed Services:** Managed MLOps platforms (AWS Sage‚ÄëMaker, Azure ML, Google Vertex AI, etc.) handle training pipelines, versioning, A/B testing, and CI/CD for models.<br>‚Ä¢ **Pre‚Äëbuilt Tools & APIs:** Access to ready‚Äëmade algorithms, feature stores, and hyper‚Äëparameter tuning services speeds up experimentation.<br>‚Ä¢ **Collaboration:** Centralized repositories and role‚Äëbased access make it easy for data scientists, engineers, and product teams to share code, datasets, and model artifacts. |\n",
      "| **3** | **Robust Security, Reliability & Compliance** | ‚Ä¢ **Enterprise‚Äëgrade Security:** Cloud providers invest heavily in encryption at rest/in‚Äëflight, IAM controls, secret management, and automated threat detection.<br>‚Ä¢ **High Availability & Disaster Recovery:** Built‚Äëin redundancy, automated backups, and multi‚Äëzone failover keep models serving even during hardware outages.<br>‚Ä¢ **Compliance Certifications:** ISO‚ÄØ27001, SOC‚ÄØ2, GDPR, HIPAA, etc., are often already attained, simplifying regulatory audits for your organization. |\n",
      "\n",
      "---\n",
      "\n",
      "### Quick Takeaway\n",
      "\n",
      "1. **Scale on demand** ‚Üí handle any workload without upfront hardware costs.  \n",
      "2. **Ship faster** ‚Üí leverage managed ML services and collaborative tooling to get from prototype to production in days, not months.  \n",
      "3. **Stay secure & reliable** ‚Üí benefit from the provider‚Äôs security expertise, uptime guarantees, and compliance frameworks.\n",
      "\n",
      "These three pillars‚Äî**elastic scalability, accelerated delivery, and enterprise‚Äëgrade trust**‚Äîare why most forward‚Äëlooking organizations move their AI/ML workloads to the cloud. üöÄ\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'**Top\\u202f3 Advantages of Using Cloud‚ÄëBased Models**\\n\\n| # | Advantage | Why It Matters (Key Benefits) |\\n|---|------------|------------------------------|\\n| **1** | **Scalable, On‚ÄëDemand Resources** | ‚Ä¢ **Elasticity:** Compute, storage, and networking can be spun up or down in seconds to match the workload (e.g., spikes in inference traffic).<br>‚Ä¢ **Pay‚Äëas‚Äëyou‚Äëgo:** You‚Äôre billed only for what you actually use, eliminating the need for costly over‚Äëprovisioned hardware.<br>‚Ä¢ **Global Reach:** Multi‚Äëregion deployments let you place models close to end‚Äëusers, reducing latency and improving user experience. |\\n| **2** | **Accelerated Development & Deployment** | ‚Ä¢ **Managed Services:** Managed MLOps platforms (AWS Sage‚ÄëMaker, Azure ML, Google Vertex AI, etc.) handle training pipelines, versioning, A/B testing, and CI/CD for models.<br>‚Ä¢ **Pre‚Äëbuilt Tools & APIs:** Access to ready‚Äëmade algorithms, feature stores, and hyper‚Äëparameter tuning services speeds up experimentation.<br>‚Ä¢ **Collaboration:** Centralized repositories and role‚Äëbased access make it easy for data scientists, engineers, and product teams to share code, datasets, and model artifacts. |\\n| **3** | **Robust Security, Reliability & Compliance** | ‚Ä¢ **Enterprise‚Äëgrade Security:** Cloud providers invest heavily in encryption at rest/in‚Äëflight, IAM controls, secret management, and automated threat detection.<br>‚Ä¢ **High Availability & Disaster Recovery:** Built‚Äëin redundancy, automated backups, and multi‚Äëzone failover keep models serving even during hardware outages.<br>‚Ä¢ **Compliance Certifications:** ISO\\u202f27001, SOC\\u202f2, GDPR, HIPAA, etc., are often already attained, simplifying regulatory audits for your organization. |\\n\\n---\\n\\n### Quick Takeaway\\n\\n1. **Scale on demand** ‚Üí handle any workload without upfront hardware costs.  \\n2. **Ship faster** ‚Üí leverage managed ML services and collaborative tooling to get from prototype to production in days, not months.  \\n3. **Stay secure & reliable** ‚Üí benefit from the provider‚Äôs security expertise, uptime guarantees, and compliance frameworks.\\n\\nThese three pillars‚Äî**elastic scalability, accelerated delivery, and enterprise‚Äëgrade trust**‚Äîare why most forward‚Äëlooking organizations move their AI/ML workloads to the cloud. üöÄ'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_basic_chat(model: str = DEFAULT_MODEL, prompt: str = \"What are the top 3 advantages of using cloud models?\") -> str:\n",
    "    \"\"\"\n",
    "    Test basic chat capability without streaming.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to use (e.g., 'gpt-oss:120b', 'qwen3')\n",
    "        prompt: The prompt to send to the model\n",
    "    \n",
    "    Returns:\n",
    "        The complete response from the model\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Sending prompt to {model}...\")\n",
    "    print(f\"   Prompt: {prompt}\\n\")\n",
    "    \n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    answer = response.message.content\n",
    "    print(f\"‚úì Response received:\\n{answer}\\n\")\n",
    "    return answer\n",
    "\n",
    "# Run basic chat test\n",
    "test_basic_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
