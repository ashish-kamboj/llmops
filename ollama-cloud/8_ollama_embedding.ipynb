{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9235bf7d",
   "metadata": {},
   "source": [
    "### Installation\n",
    "```bash\n",
    "pip install ollama chromadb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684fad2d",
   "metadata": {},
   "source": [
    "### Step 1: Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91694e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "\n",
    "documents = [\n",
    "  \"Llamas are members of the camelid family meaning they're pretty closely related to vicuÃ±as and camels\",\n",
    "  \"Llamas were first domesticated and used as pack animals 4,000 to 5,000 years ago in the Peruvian highlands\",\n",
    "  \"Llamas can grow as much as 6 feet tall though the average llama between 5 feet 6 inches and 5 feet 9 inches tall\",\n",
    "  \"Llamas weigh between 280 and 450 pounds and can carry 25 to 30 percent of their body weight\",\n",
    "  \"Llamas are vegetarians and have very efficient digestive systems\",\n",
    "  \"Llamas live to be about 20 years old, though some only live for 15 years and others live to be 30 years old\",\n",
    "]\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"docs\")\n",
    "\n",
    "# store each document in a vector embedding database\n",
    "for i, d in enumerate(documents):\n",
    "  response = ollama.embed(model=\"mxbai-embed-large\", input=d)\n",
    "  embeddings = response[\"embeddings\"]\n",
    "  collection.add(\n",
    "    ids=[str(i)],\n",
    "    embeddings=embeddings,\n",
    "    documents=[d]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a6c34",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve\n",
    "Retrieve the most relevant document given an example prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47944c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.embed() got an unexpected keyword argument 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mWhat animals are llamas related to?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# generate an embedding for the input and retrieve the most relevant doc\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m response = \u001b[43mollama\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmxbai-embed-large\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m results = collection.query(\n\u001b[32m     10\u001b[39m   query_embeddings=[response[\u001b[33m\"\u001b[39m\u001b[33membeddings\u001b[39m\u001b[33m\"\u001b[39m]],\n\u001b[32m     11\u001b[39m   n_results=\u001b[32m1\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     13\u001b[39m data = results[\u001b[33m'\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n",
      "\u001b[31mTypeError\u001b[39m: Client.embed() got an unexpected keyword argument 'prompt'"
     ]
    }
   ],
   "source": [
    "# an example input\n",
    "prompt = \"What animals are llamas related to?\"\n",
    "\n",
    "# generate an embedding for the input and retrieve the most relevant doc\n",
    "response = ollama.embed(\n",
    "  model=\"mxbai-embed-large\",\n",
    "  input=prompt\n",
    ")\n",
    "results = collection.query(\n",
    "  query_embeddings=[response[\"embeddings\"]],\n",
    "  n_results=1\n",
    ")\n",
    "data = results['documents'][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fecf8f7",
   "metadata": {},
   "source": [
    "### Step 3: Generate\n",
    "Use the prompt and the document retrieved in the previous step to generate an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751f591",
   "metadata": {},
   "source": [
    "# generate a response combining the prompt and data we retrieved in step 2\n",
    "output = ollama.generate(\n",
    "  model=\"llama2\",\n",
    "  prompt=f\"Using this data: {data}. Respond to this prompt: {input}\"\n",
    ")\n",
    "\n",
    "print(output['response'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
