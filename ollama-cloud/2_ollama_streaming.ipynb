{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c9b33c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an Ollama account at [ollama.com](https://ollama.com)\n",
    "2. Generate an API key from [ollama.com/settings/keys](https://ollama.com/settings/keys)\n",
    "3. Set the API key in the `.env` file in this folder\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install ollama python-dotenv requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2453f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing python-dotenv...\n",
      "âœ“ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages and import libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['ollama', 'python-dotenv', 'requests']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"âœ“ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e734ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Ollama API configured\n",
      "  Endpoint: https://ollama.com\n",
      "  Default Model: gpt-oss:120b\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use Path.cwd() since __file__ is not defined in Jupyter notebooks\n",
    "env_path = Path.cwd() / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    # Try to find .env in the ollama-cloud directory\n",
    "    load_dotenv(dotenv_path='d:\\\\ollama-n8n\\\\ollama-cloud\\\\.env')\n",
    "\n",
    "# Get API key and configuration\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "OLLAMA_ENDPOINT = os.getenv('OLLAMA_API_ENDPOINT', 'https://ollama.com')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'gpt-oss:120b')\n",
    "\n",
    "# Verify API key is set\n",
    "if not OLLAMA_API_KEY or OLLAMA_API_KEY == 'your_api_key_here':\n",
    "    print(\"âš ï¸  WARNING: OLLAMA_API_KEY not set in .env file!\")\n",
    "    print(\"Please set your API key in the .env file: https://ollama.com/settings/keys\")\n",
    "else:\n",
    "    print(f\"âœ“ Ollama API configured\")\n",
    "    print(f\"  Endpoint: {OLLAMA_ENDPOINT}\")\n",
    "    print(f\"  Default Model: {DEFAULT_MODEL}\")\n",
    "\n",
    "# Initialize Ollama client for cloud API\n",
    "client = ollama.Client(\n",
    "    host=OLLAMA_ENDPOINT,\n",
    "    headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046e342a",
   "metadata": {},
   "source": [
    "## Streaming Capability - Real-time Response\n",
    "\n",
    "Stream responses token-by-token for real-time interaction and lower latency perception."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c81cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒŠ Streaming response from gpt-oss:120b...\n",
      "   Prompt: Write a haiku about artificial intelligence\n",
      "\n",
      "   Response: Silent circuits hum  \n",
      "Learning thoughts beyond our grasp  \n",
      "Dreams of silicon\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Silent circuits hum  \\nLearning thoughts beyond our grasp  \\nDreams of silicon'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_streaming_chat(model: str = DEFAULT_MODEL, prompt: str = \"Write a haiku about artificial intelligence\") -> str:\n",
    "    \"\"\"\n",
    "    Test streaming chat - tokens arrive in real-time.\n",
    "    \n",
    "    Args:\n",
    "        model: Model to use\n",
    "        prompt: The prompt to stream\n",
    "    \n",
    "    Returns:\n",
    "        The complete response\n",
    "    \"\"\"\n",
    "    print(f\"ðŸŒŠ Streaming response from {model}...\")\n",
    "    print(f\"   Prompt: {prompt}\\n\")\n",
    "    print(\"   Response: \", end='', flush=True)\n",
    "    \n",
    "    full_response = \"\"\n",
    "    \n",
    "    # Stream the response\n",
    "    stream = client.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': prompt}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        if chunk.message.content:\n",
    "            content = chunk.message.content\n",
    "            print(content, end='', flush=True)\n",
    "            full_response += content\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    return full_response\n",
    "\n",
    "# Run streaming test\n",
    "test_streaming_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
