{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20da616",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an Ollama account at [ollama.com](https://ollama.com)\n",
    "2. Generate an API key from [ollama.com/settings/keys](https://ollama.com/settings/keys)\n",
    "3. Set the API key in the `.env` file in this folder\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install ollama python-dotenv requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2453f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing python-dotenv...\n",
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages and import libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['ollama', 'python-dotenv', 'requests']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e734ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama API configured\n",
      "  Endpoint: https://ollama.com\n",
      "  Default Model: gpt-oss:120b\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use Path.cwd() since __file__ is not defined in Jupyter notebooks\n",
    "env_path = Path.cwd() / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    # Try to find .env in the ollama-cloud directory\n",
    "    load_dotenv(dotenv_path='d:\\\\ollama-n8n\\\\ollama-cloud\\\\.env')\n",
    "\n",
    "# Get API key and configuration\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "OLLAMA_ENDPOINT = os.getenv('OLLAMA_API_ENDPOINT', 'https://ollama.com')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'gpt-oss:120b')\n",
    "\n",
    "# Verify API key is set\n",
    "if not OLLAMA_API_KEY or OLLAMA_API_KEY == 'your_api_key_here':\n",
    "    print(\"  WARNING: OLLAMA_API_KEY not set in .env file!\")\n",
    "    print(\"Please set your API key in the .env file: https://ollama.com/settings/keys\")\n",
    "else:\n",
    "    print(f\"✓ Ollama API configured\")\n",
    "    print(f\"  Endpoint: {OLLAMA_ENDPOINT}\")\n",
    "    print(f\"  Default Model: {DEFAULT_MODEL}\")\n",
    "\n",
    "# Initialize Ollama client for cloud API\n",
    "client = ollama.Client(\n",
    "    host=OLLAMA_ENDPOINT,\n",
    "    headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aef76d",
   "metadata": {},
   "source": [
    "## Advanced: Building an AI Agent with Web Search\n",
    "\n",
    "Combine thinking, tool calling, and web search to create an intelligent agent that can research topics and provide comprehensive answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2ef9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AI RESEARCH AGENT - Web Search & Thinking Combination\n",
      "======================================================================\n",
      "\n",
      "   Building AI Research Agent\n",
      "   Model: gpt-oss:120b\n",
      "   Query: 'What is Ollama and what are its key advantages?'\n",
      "\n",
      "Agent reasoning process...\n",
      "\n",
      "[Iteration 1]\n",
      "  Thinking: The user asks: \"What is Ollama and what are its key advantages?\" Need to provide a comprehensive answer. Probably about Ollama, an open-source platform for running LLMs locally. Provide description, f...\n",
      "  Response: **Ollama – at a glance**\n",
      "\n",
      "Ollama is an open‑source platform that makes it painless to run Large Language Models (LLMs) on your own hardware (desktop, laptop, on‑prem server, or cloud VM). It bundles a...\n",
      "\n",
      "✓ Agent completed research\n",
      "\n",
      "Final Answer:\n",
      "**Ollama – at a glance**\n",
      "\n",
      "Ollama is an open‑source platform that makes it painless to run Large Language Models (LLMs) on your own hardware (desktop, laptop, on‑prem server, or cloud VM). It bundles a lightweight model‑serving daemon, a simple command‑line client, and a set of utilities for downloading, managing, and interacting with dozens of pre‑converted LLMs (Llama 2, Mistral, Gemma, etc.).  \n",
      "\n",
      "In short, Ollama lets you:\n",
      "\n",
      "| What it does | How it works |\n",
      "|--------------|--------------|\n",
      "| **Download** ready‑to‑run LLMs (quantised GGML/gguf files) with a single command | The daemon fetches models from the Ollama model hub, caches them locally, and prepares them for fast inference. |\n",
      "| **Serve** models over a local HTTP / gRPC API that mimics the OpenAI‑style `/v1/chat/completions` endpoint | Any client that can talk to an OpenAI‑compatible API (e.g., LangChain, LlamaIndex, Postman, cURL) can use Ollama without code changes. |\n",
      "| **Interact** via a tidy terminal UI (`ollama run <model>`) or programmatically (`curl`, Python SDK, etc.) | The CLI provides an REPL‑like chat experience; the API lets you embed the model in apps, notebooks, or micro‑services. |\n",
      "| **Manage** multiple models, switch versions, and pin specific releases | Commands such as `ollama list`, `ollama pull`, `ollama rm` keep the local model store tidy. |\n",
      "\n",
      "---\n",
      "\n",
      "## Key Advantages\n",
      "\n",
      "### 1. **Full data privacy & security**\n",
      "- **Local‑only execution** – No network request leaves the machine unless you explicitly call external services.  \n",
      "- **Zero data‑exfiltration risk** – Ideal for regulated industries (healthcare, finance, government) that cannot send user prompts to third‑party APIs.\n",
      "\n",
      "### 2. **Cost‑effectiveness**\n",
      "- **No per‑token fees** – Once you have the hardware, you pay only for electricity and storage.  \n",
      "- **Free open‑source models** – Many powerful LLMs (e.g., Llama 2, Mistral‑7B) are available at no cost, avoiding the hefty commercial API pricing.\n",
      "\n",
      "### 3. **Performance & latency**\n",
      "- **GPU‑accelerated inference** (CUDA, Metal, Vulkan) reduces response times dramatically compared with remote APIs that suffer network jitter.  \n",
      "- **CPU fallback** – Works on laptops without a GPU, using quantised models (e.g., 4‑bit GGUF) that still run comfortably.\n",
      "\n",
      "### 4. **Simplicity of setup**\n",
      "- **Single binary install** (`curl -fsSL https://ollama.com/install.sh | sh` on Linux/macOS, or a native installer for Windows) – no Python virtual environments, Docker, or complex dependencies required.  \n",
      "- **One‑line model pull** – `ollama pull llama2:13b` pulls, converts, and registers the model automatically.\n",
      "\n",
      "### 5. **Compatibility with existing tooling**\n",
      "- **OpenAI‑compatible REST endpoints** – Drop‑in replacement for `openai` Python library, LangChain, LlamaIndex, HuggingFace’s `transformers`, etc.  \n",
      "- **Standardised JSON schema** – Same request/response shape as `/v1/chat/completions`, `v1/completions`, `v1/embeddings`, etc.\n",
      "\n",
      "### 6. **Extensible model ecosystem**\n",
      "- **Model hub** – Central repository of community‑curated, pre‑quantised models with version tags.  \n",
      "- **Custom model import** – You can add any GGUF/ggml model you have compiled yourself (e.g., fine‑tuned LLaMA, private corporate models).  \n",
      "\n",
      "### 7. **Cross‑platform support**\n",
      "- **Linux, macOS, Windows**, plus **ARM** (Apple Silicon, Raspberry Pi) – Ollama builds native binaries for each major OS/architecture.\n",
      "\n",
      "### 8. **Developer‑friendly tooling**\n",
      "- **CLI chat UI** (`ollama run <model>`) for quick prototyping.  \n",
      "- **Programmatic SDKs** (`ollama-python`, `ollama-js`) for deeper integration.  \n",
      "- **Built‑in logging & metrics** (stdout logs, Prometheus exporter) for observability in production.\n",
      "\n",
      "### 9. **Version control & reproducibility**\n",
      "- **Pinned model tags** (`ollama pull mistral:7b-instruct-v0.3`) guarantee that the same weights are used across dev, test, and prod environments.  \n",
      "- **Model removal** (`ollama rm <model>`) frees up disk space, supporting CI pipelines that spin up fresh environments.\n",
      "\n",
      "### 10. **Active community & commercial backing**\n",
      "- **Open‑source core** with regular releases & issue triage on GitHub.  \n",
      "- **Enterprise options** (Ollama Enterprise, hosted SaaS) for organisations that need managed deployments, SSO, and support.\n",
      "\n",
      "---\n",
      "\n",
      "## Typical Use‑Cases\n",
      "\n",
      "| Scenario | Why Ollama shines |\n",
      "|----------|-------------------|\n",
      "| **Prototyping LLM‑driven features** | Instantly spin up a local chat model; no API keys or network latency. |\n",
      "| **Secure document analysis** | Keep all confidential docs on‑prem, feed them to a local model without risking data leaks. |\n",
      "| **Edge devices & offline apps** | Run a 7B model on a laptop or Raspberry Pi for field work where internet is unavailable. |\n",
      "| **Cost‑sensitive scaling** | Deploy a pool of GPU servers serving Ollama endpoints; static cost vs. variable API bills. |\n",
      "| **Research & fine‑tuning** | Load custom‑trained GGUF weights, test inference, compare variants side‑by‑side. |\n",
      "| **Integration into existing pipelines** | Swap `openai.ChatCompletion.create` with `ollama.ChatCompletion.create` – minimal code changes. |\n",
      "\n",
      "---\n",
      "\n",
      "## Quick “Hello World” Example\n",
      "\n",
      "```bash\n",
      "# 1️⃣ Install Ollama (one‑liner)\n",
      "curl -fsSL https://ollama.com/install.sh | sh   # Linux/macOS\n",
      "# or download the Windows installer from https://ollama.com\n",
      "\n",
      "# 2️⃣ Pull a model (e.g., Llama 2 7B)\n",
      "ollama pull llama2\n",
      "\n",
      "# 3️⃣ Chat interactively\n",
      "ollama run llama2\n",
      "```\n",
      "\n",
      "Programmatic use (Python):\n",
      "\n",
      "```python\n",
      "import ollama\n",
      "\n",
      "resp = ollama.chat(\n",
      "    model=\"llama2\",\n",
      "    messages=[{\"role\": \"user\", \"content\": \"Explain quantum entanglement in two sentences.\"}]\n",
      ")\n",
      "\n",
      "print(resp[\"message\"][\"content\"])\n",
      "```\n",
      "\n",
      "This works because `ollama.chat` mirrors the `openai.ChatCompletion.create` signature.\n",
      "\n",
      "---\n",
      "\n",
      "## TL;DR Summary\n",
      "\n",
      "- **Ollama** = a lightweight, open‑source server + CLI that lets you run LLMs locally with a simple, OpenAI‑compatible API.  \n",
      "- **Key advantages**: privacy (data never leaves your machine), zero per‑token cost, low latency, easy installation, GPU/CPU flexibility, compatibility with existing LLM tooling, and a growing library of ready‑to‑use models.  \n",
      "\n",
      "If you need an LLM that you can control, audit, and run without paying recurring cloud fees, Ollama is currently one of the most straightforward ways to get there.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_ai_agent_with_search(model: str = 'gpt-oss:120b', query: str = \"What are the latest developments in Ollama as of 2025?\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Advanced: Build an AI agent that uses web search and thinking to answer questions.\n",
    "    \n",
    "    Args:\n",
    "        model: Model with thinking support and tool access\n",
    "        query: Question for the agent to research\n",
    "    \n",
    "    Returns:\n",
    "        Agent's researched response\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"   Building AI Research Agent\")\n",
    "    print(f\"   Model: {model}\")\n",
    "    print(f\"   Query: '{query}'\\n\")\n",
    "    \n",
    "    # Define available tools for the agent\n",
    "    available_tools = {\n",
    "        'web_search': ollama.web_search,\n",
    "        'web_fetch': ollama.web_fetch\n",
    "    }\n",
    "    \n",
    "    # Tool definitions for the model\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"web_search\",\n",
    "                \"description\": \"Search the web for information\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"Search query\"},\n",
    "                        \"max_results\": {\"type\": \"integer\", \"description\": \"Max results (default 3)\"}\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"web_fetch\",\n",
    "                \"description\": \"Fetch content from a specific URL\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"url\": {\"type\": \"string\", \"description\": \"URL to fetch\"}\n",
    "                    },\n",
    "                    \"required\": [\"url\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    messages = [{'role': 'user', 'content': query}]\n",
    "    \n",
    "    # Think level for GPT-OSS, boolean for others\n",
    "    think_param = 'medium' if 'gpt-oss' in model else True\n",
    "    \n",
    "    print(f\"Agent reasoning process...\\n\")\n",
    "    \n",
    "    # Main agent loop\n",
    "    iteration = 0\n",
    "    max_iterations = 3  # Prevent infinite loops\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"[Iteration {iteration}]\")\n",
    "        \n",
    "        # Get response with tools enabled\n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            think=think_param,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Show thinking if available\n",
    "        if hasattr(response.message, 'thinking') and response.message.thinking:\n",
    "            thinking = response.message.thinking[:200] + \"...\" if len(response.message.thinking) > 200 else response.message.thinking\n",
    "            print(f\"  Thinking: {thinking}\")\n",
    "        \n",
    "        # Show content\n",
    "        if response.message.content:\n",
    "            print(f\"  Response: {response.message.content[:200]}...\")\n",
    "        \n",
    "        # Check for tool calls\n",
    "        if hasattr(response.message, 'tool_calls') and response.message.tool_calls:\n",
    "            print(f\"  Tools to call: {len(response.message.tool_calls)}\")\n",
    "            \n",
    "            # Append assistant's response\n",
    "            messages.append(response.message)\n",
    "            \n",
    "            # Execute tools\n",
    "            for tool_call in response.message.tool_calls:\n",
    "                func_name = tool_call.function.name\n",
    "                func_args = tool_call.function.arguments\n",
    "                \n",
    "                print(f\"    → Calling {func_name}...\")\n",
    "                \n",
    "                try:\n",
    "                    # Call the function\n",
    "                    if func_name == 'web_search':\n",
    "                        result = client.web_search(\n",
    "                            func_args.get('query', ''),\n",
    "                            max_results=func_args.get('max_results', 3)\n",
    "                        )\n",
    "                        result_str = str(result)[:1500]  # Limit context\n",
    "                    elif func_name == 'web_fetch':\n",
    "                        result = client.web_fetch(func_args.get('url', ''))\n",
    "                        result_str = str(result)[:1500]  # Limit context\n",
    "                    else:\n",
    "                        result_str = \"Tool not found\"\n",
    "                    \n",
    "                    # Add tool result to conversation\n",
    "                    messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'content': result_str,\n",
    "                        'tool_name': func_name\n",
    "                    })\n",
    "                    print(f\"      Result: {result_str[:100]}...\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"      Error: {str(e)}\")\n",
    "                    messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'content': f\"Error: {str(e)}\",\n",
    "                        'tool_name': func_name\n",
    "                    })\n",
    "        \n",
    "        else:\n",
    "            # No more tool calls, agent has completed\n",
    "            print(\"\\n✓ Agent completed research\\n\")\n",
    "            break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Return final response\n",
    "    final_content = response.message.content if response.message.content else \"Agent completed\"\n",
    "    print(f\"Final Answer:\\n{final_content}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'query': query,\n",
    "        'iterations': iteration,\n",
    "        'final_response': final_content\n",
    "    }\n",
    "\n",
    "# Run the AI agent\n",
    "print(\"=\" * 70)\n",
    "print(\"AI RESEARCH AGENT - Web Search & Thinking Combination\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "agent_result = test_ai_agent_with_search(\n",
    "    query=\"What is Ollama and what are its key advantages?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
