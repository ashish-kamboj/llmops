{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5c9b33c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Prerequisites\n",
    "1. Create an Ollama account at [ollama.com](https://ollama.com)\n",
    "2. Generate an API key from [ollama.com/settings/keys](https://ollama.com/settings/keys)\n",
    "3. Set the API key in the `.env` file in this folder\n",
    "\n",
    "### Installation\n",
    "```bash\n",
    "pip install ollama python-dotenv requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2453f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing python-dotenv...\n",
      "✓ All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Setup: Install required packages and import libraries\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "packages = ['ollama', 'python-dotenv', 'requests']\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "print(\"✓ All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e734ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ollama API configured\n",
      "  Endpoint: https://ollama.com\n",
      "  Default Model: gpt-oss:120b\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "import base64\n",
    "\n",
    "# Load environment variables from .env file\n",
    "# Use Path.cwd() since __file__ is not defined in Jupyter notebooks\n",
    "env_path = Path.cwd() / '.env'\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "else:\n",
    "    # Try to find .env in the ollama-cloud directory\n",
    "    load_dotenv(dotenv_path='d:\\\\ollama-n8n\\\\ollama-cloud\\\\.env')\n",
    "\n",
    "# Get API key and configuration\n",
    "OLLAMA_API_KEY = os.getenv('OLLAMA_API_KEY')\n",
    "OLLAMA_ENDPOINT = os.getenv('OLLAMA_API_ENDPOINT', 'https://ollama.com')\n",
    "DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'gpt-oss:120b')\n",
    "\n",
    "# Verify API key is set\n",
    "if not OLLAMA_API_KEY or OLLAMA_API_KEY == 'your_api_key_here':\n",
    "    print(\" WARNING: OLLAMA_API_KEY not set in .env file!\")\n",
    "    print(\"Please set your API key in the .env file: https://ollama.com/settings/keys\")\n",
    "else:\n",
    "    print(f\"✓ Ollama API configured\")\n",
    "    print(f\"  Endpoint: {OLLAMA_ENDPOINT}\")\n",
    "    print(f\"  Default Model: {DEFAULT_MODEL}\")\n",
    "\n",
    "# Initialize Ollama client for cloud API\n",
    "client = ollama.Client(\n",
    "    host=OLLAMA_ENDPOINT,\n",
    "    headers={'Authorization': f'Bearer {OLLAMA_API_KEY}'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e10cf",
   "metadata": {},
   "source": [
    "## Vision Capability - Analyze Images\n",
    "\n",
    "Some Ollama models support vision/multimodal capabilities. They can analyze images and answer questions about them.\n",
    "\n",
    "**Note**: This example uses base64 encoding. You can also use image URLs directly in many cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db3ed690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing vision with online image:\n",
      "\n",
      "   Testing vision capability with gemma3:4b...\n",
      "   Image URL: https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg\n",
      "   Downloading image...\n",
      "   ✓ Image downloaded (91814 bytes, image/jpeg)\n",
      "\n",
      "Vision Analysis:\n",
      "Okay, here’s a detailed description of what I see in the image:\n",
      "\n",
      "**Overall Impression:**\n",
      "\n",
      "The image is a portrait, likely taken in the late 1960s or early 1970s, judging by the fashion and the color palette. It has a slightly vintage, possibly slightly faded, feel due to the color processing. The lighting is warm and creates a soft, flattering effect.\n",
      "\n",
      "**The Subject:**\n",
      "\n",
      "*   **Woman:** A young woman is the central focus. She has a relaxed, contemplative expression with her eyes looking slightly to the right.\n",
      "*   **Hair:** Her hair is long, wavy, and a rich, dark brown color. A strand of hair is draped dramatically over the side of her hat, adding a touch of bohemian style.\n",
      "*   **Skin Tone:** She appears to have fair skin with a warm undertone.\n",
      "\n",
      "**Clothing and Accessories:**\n",
      "\n",
      "*   **Hat:** The most striking element is her hat. It’s a light beige or cream-colored straw hat with a wide brim. It has a decorative band of dark blue feathers or plumes extending out from the front. The hat is shaped with a slight curve and a small, pointed crown.\n",
      "*   **Dress/Top:** She's wearing a sleeveless, strapless dress or top that is a light, pale peach or cream color.  It appears to be quite simple.\n",
      "*   **Necklace:** A delicate gold necklace is visible, adding a touch of elegance.\n",
      "\n",
      "**Background and Composition:**\n",
      "\n",
      "*   **Background:** The background is blurred and warm-toned, predominantly shades of peach and orange. There seems to be an architectural element—perhaps the corner of a building or a curved wall—with a dark, rectangular shape (possibly a mirror) to the right of the woman’s head. \n",
      "*   **Framing:** The image is a close-up portrait, focusing primarily on the woman’s face and shoulders. The composition is somewhat centered. \n",
      "\n",
      "**Color and Tone:**\n",
      "\n",
      "*   The image has a warm, slightly saturated color palette. The tones are rich and contribute to the retro feel. There's a noticeable warmth to the skin tones and the background.\n",
      "\n",
      "**Overall Style:**\n",
      "\n",
      "The photograph has a classic, glamorous style reminiscent of fashion photography from the 1960s/70s. It's a beautifully posed portrait, emphasizing the woman’s beauty and the fashionable accessories.\n",
      "\n",
      "Do you want me to focus on any specific aspect of the image, such as the hat, the lighting, or the woman’s expression?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_vision_with_url(model: str = 'gemma3:4b', image_url: str = 'https://raw.githubusercontent.com/opencv/opencv/master/samples/data/lena.jpg') -> str:\n",
    "    \"\"\"\n",
    "    Test vision capability using an image URL.\n",
    "    Downloads the image and converts to base64 for Ollama.\n",
    "    \n",
    "    Args:\n",
    "        model: Vision-capable model (llama4:scout, qwen2.5vl, etc.)\n",
    "        image_url: URL of the image to analyze\n",
    "    \n",
    "    Returns:\n",
    "        Model's analysis of the image\n",
    "    \"\"\"\n",
    "    print(f\"   Testing vision capability with {model}...\")\n",
    "    print(f\"   Image URL: {image_url}\")\n",
    "    print(f\"   Downloading image...\")\n",
    "    \n",
    "    try:\n",
    "        # Download image from URL with proper headers to avoid 403 errors\n",
    "        import requests\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        }\n",
    "        response_img = requests.get(image_url, headers=headers, timeout=10, allow_redirects=True)\n",
    "        response_img.raise_for_status()\n",
    "        \n",
    "        # Verify we got image data, not HTML\n",
    "        content_type = response_img.headers.get('content-type', '')\n",
    "        if 'text/html' in content_type or not content_type.startswith('image'):\n",
    "            print(f\"   Warning: Received {content_type} instead of image\")\n",
    "            print(f\"   Response preview: {response_img.content[:100]}\")\n",
    "            raise ValueError(f\"URL returned {content_type} instead of an image\")\n",
    "        \n",
    "        # Convert to base64 - Ollama expects just the base64 string, not data URI\n",
    "        image_data = base64.b64encode(response_img.content).decode('utf-8')\n",
    "        \n",
    "        print(f\"   ✓ Image downloaded ({len(response_img.content)} bytes, {content_type})\\n\")\n",
    "        \n",
    "        response = client.chat(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\n",
    "                    'role': 'user',\n",
    "                    'content': 'Describe what you see in this image in detail.',\n",
    "                    'images': [image_data]  # Pass base64 string directly, not data URI\n",
    "                }\n",
    "            ],\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        answer = response.message.content\n",
    "        print(f\"Vision Analysis:\\n{answer}\\n\")\n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        print(\"Note: Make sure the URL is accessible and the model supports vision.\")\n",
    "        raise\n",
    "\n",
    "def test_vision_with_base64(model: str = 'gemma3:4b', image_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Test vision capability with local image (base64 encoded).\n",
    "    \n",
    "    Args:\n",
    "        model: Vision-capable model\n",
    "        image_path: Path to local image file\n",
    "    \n",
    "    Returns:\n",
    "        Model's analysis\n",
    "    \"\"\"\n",
    "    if image_path is None:\n",
    "        print(\"   Note: For this example to work, provide a path to a local image file.\")\n",
    "        print(\"   Example: test_vision_with_base64(image_path='./path/to/image.jpg')\\n\")\n",
    "        return \"No image provided\"\n",
    "    \n",
    "    # Read and encode image\n",
    "    with open(image_path, 'rb') as f:\n",
    "        image_data = base64.b64encode(f.read()).decode('utf-8')\n",
    "    \n",
    "    # Determine image format\n",
    "    file_ext = Path(image_path).suffix.lower()\n",
    "    media_type_map = {\n",
    "        '.jpg': 'image/jpeg',\n",
    "        '.jpeg': 'image/jpeg',\n",
    "        '.png': 'image/png',\n",
    "        '.gif': 'image/gif',\n",
    "        '.webp': 'image/webp'\n",
    "    }\n",
    "    media_type = media_type_map.get(file_ext, 'image/jpeg')\n",
    "    \n",
    "    print(f\"   Testing vision with local image...\")\n",
    "    print(f\"   Model: {model}\")\n",
    "    print(f\"   Image: {image_path}\\n\")\n",
    "    \n",
    "    response = client.chat(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': 'Analyze this image and describe what you see.',\n",
    "                'images': [image_data]  # Pass base64 string directly, not data URI\n",
    "            }\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "    \n",
    "    answer = response.message.content\n",
    "    print(f\"Vision Analysis:\\n{answer}\\n\")\n",
    "    return answer\n",
    "\n",
    "# Test vision with URL\n",
    "print(\"Testing vision with online image:\\n\")\n",
    "vision_result = test_vision_with_url()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
