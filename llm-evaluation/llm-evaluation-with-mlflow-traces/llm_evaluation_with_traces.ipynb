{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f69569",
   "metadata": {},
   "source": [
    "# LLM Evaluation Using MLflow Traces and Datasets\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates how to:\n",
    "1. **Simulate real-world chat interactions** between users and an LLM\n",
    "2. **Capture traces** of each conversation turn using MLflow\n",
    "3. **Add expectations** (ground truth annotations) to traces\n",
    "4. **Build evaluation datasets** from annotated traces\n",
    "5. **Evaluate LLM responses** systematically using MLflow's evaluation framework\n",
    "\n",
    "## Why This Approach?\n",
    "Building datasets from traces allows you to:\n",
    "- **Capture real interactions**: Record actual user-LLM conversations\n",
    "- **Evolve your test suite**: Continuously add new test cases from production\n",
    "- **Systematic evaluation**: Measure performance consistently across iterations\n",
    "- **Track improvements**: Compare model versions against the same test data\n",
    "\n",
    "## The Evaluation Loop\n",
    "```\n",
    "User Question ‚Üí LLM Response ‚Üí Capture Trace ‚Üí Add Expectations ‚Üí \n",
    "Build Dataset ‚Üí Run Evaluation ‚Üí Analyze Results ‚Üí Iterate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303cd084",
   "metadata": {},
   "source": [
    "                \"## Setup and Imports\",\n",
    "                \"\",\n",
    "                \"**üéØ Configuration-Driven Approach:**\",\n",
    "                \"This notebook uses `config.yaml` for all parameters. You can customize:\",\n",
    "                \"- Groq models and API settings\",\n",
    "                \"- A/B testing configuration\",\n",
    "                \"- Sample questions and expected answers\",\n",
    "                \"- Evaluation metrics and thresholds\",\n",
    "                \"- Visualization settings\",\n",
    "                \"\",\n",
    "                \"**No need to modify notebook code - just edit config.yaml!**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73fb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install mlflow groq pandas numpy matplotlib seaborn rouge-score nltk python-dotenv scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb4b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.genai.datasets import create_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from groq import Groq\n",
    "import yaml\n",
    "\n",
    "# For advanced evaluation metrics\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"üìÅ Configuration loaded from config.yaml\")\n",
    "print(f\"   - Models available: {', '.join(config['groq']['models'].keys())}\")\n",
    "print(f\"   - A/B Testing: {'Enabled' if config['ab_testing']['enabled'] else 'Disabled'}\")\n",
    "print(f\"   - Evaluation metrics: {len([k for k, v in config['evaluation']['metrics'].items() if v.get('enabled', True)])} enabled\")\n",
    "\n",
    "# Set visualization style from config\n",
    "sns.set_style(config['visualization']['style'])\n",
    "\n",
    "# Set MLflow tracking URI from config\n",
    "mlflow.set_tracking_uri(config['mlflow']['tracking_uri'])\n",
    "\n",
    "# Create or get experiment from config\n",
    "experiment_name = config['mlflow']['experiment_name']\n",
    "experiment = mlflow.set_experiment(experiment_name)\n",
    "experiment_id = experiment.experiment_id\n",
    "\n",
    "print(f\"\\nüìä MLflow Setup:\")\n",
    "print(f\"   - Experiment: {experiment_name}\")\n",
    "print(f\"   - Experiment ID: {experiment_id}\")\n",
    "print(f\"   - Tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"\\n‚úì All imports and configuration loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82d5dd",
   "metadata": {},
   "source": [
    "                \"## Step 1: Initialize Groq API Client\",\n",
    "                \"\",\n",
    "                \"We'll use Groq API for fast LLM inference.\",\n",
    "                \"\",\n",
    "                \"**Configuration:**\",\n",
    "                \"- API key is loaded from `.env` file\",\n",
    "                \"- Available models are defined in `config.yaml`\",\n",
    "                \"- Model parameters (temperature, max_tokens, etc.) are in `config.yaml`\",\n",
    "                \"\",\n",
    "                \"**Setup**: Add your API key to `.env` file:\",\n",
    "                \"```\",\n",
    "                \"GROQ_API_KEY=your_groq_api_key_here\",\n",
    "                \"```\",\n",
    "                \"\",\n",
    "                \"Get your API key from: https://console.groq.com/\",\n",
    "                \"\",\n",
    "                \"**To change models or settings, edit config.yaml instead of code!**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e1d0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Groq client with API key from .env file\n",
    "# API key environment variable name comes from config\n",
    "groq_api_key_env = config['groq']['api_key_env']\n",
    "groq_api_key = os.getenv(groq_api_key_env)\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise ValueError(f\"{groq_api_key_env} not found in .env file. Please add it.\")\n",
    "\n",
    "groq_client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Load available Groq models from config\n",
    "GROQ_MODELS = {\n",
    "    key: details['model_id'] \n",
    "    for key, details in config['groq']['models'].items()\n",
    "}\n",
    "\n",
    "print(\"‚úì Groq client initialized\")\n",
    "print(f\"\\nü§ñ Available models for A/B testing:\")\n",
    "for name, model_id in GROQ_MODELS.items():\n",
    "    model_info = config['groq']['models'][name]\n",
    "    print(f\"  - {name}: {model_id}\")\n",
    "    print(f\"    ‚îî‚îÄ {model_info['description']}\")\n",
    "    print(f\"    ‚îî‚îÄ Recommended for: {model_info['recommended_for']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ac82db",
   "metadata": {},
   "source": [
    "## Step 2: Groq-Powered Chat Function\n",
    "\n",
    "This function:\n",
    "- Accepts user questions\n",
    "- Calls Groq API to generate responses\n",
    "- Captures each interaction as an MLflow trace\n",
    "- Supports multiple models for A/B testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35222f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groq_llm_response(question: str, model_key: str = None, temperature: float = None) -> tuple:\n",
    "    \"\"\"\n",
    "    Call Groq API to generate response using specified model.\n",
    "    Uses configuration from config.yaml for default values.\n",
    "    \n",
    "    Args:\n",
    "        question: User's input question\n",
    "        model_key: Key for model selection from GROQ_MODELS (uses config default if None)\n",
    "        temperature: Sampling temperature (uses config default if None)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response_text, model_id, tokens_used)\n",
    "    \"\"\"\n",
    "    # Use defaults from config if not provided\n",
    "    if model_key is None:\n",
    "        model_key = config['groq']['default_model']\n",
    "    if temperature is None:\n",
    "        temperature = config['groq']['default_temperature']\n",
    "    \n",
    "    model_id = GROQ_MODELS.get(model_key, GROQ_MODELS[config['groq']['default_model']])\n",
    "    \n",
    "    try:\n",
    "        # Call Groq API with settings from config\n",
    "        chat_completion = groq_client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": config['groq']['system_prompt']\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                }\n",
    "            ],\n",
    "            model=model_id,\n",
    "            temperature=temperature,\n",
    "            max_tokens=config['groq']['max_tokens'],\n",
    "            top_p=config['groq']['top_p'],\n",
    "            stream=config['groq']['stream']\n",
    "        )\n",
    "        \n",
    "        response_text = chat_completion.choices[0].message.content\n",
    "        tokens_used = chat_completion.usage.total_tokens\n",
    "        \n",
    "        return response_text, model_id, tokens_used\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Groq API: {e}\")\n",
    "        return f\"Error: {str(e)}\", model_id, 0\n",
    "\n",
    "\n",
    "@mlflow.trace(name=\"chat_completion\", span_type=\"CHAT_MODEL\")\n",
    "def chat_with_llm(user_question: str, conversation_id: str, model_key: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Chat with Groq LLM and capture the interaction as an MLflow trace.\n",
    "    Uses configuration from config.yaml for default model.\n",
    "    \n",
    "    Args:\n",
    "        user_question: The question asked by the user\n",
    "        conversation_id: Unique identifier for the conversation\n",
    "        model_key: Model to use (uses config default if None)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing question, answer, and metadata\n",
    "    \"\"\"\n",
    "    # Get response from Groq (uses config defaults if model_key is None)\n",
    "    llm_answer, model_id, tokens = groq_llm_response(user_question, model_key)\n",
    "    \n",
    "    # Prepare response with metadata\n",
    "    response = {\n",
    "        \"question\": user_question,\n",
    "        \"answer\": llm_answer,\n",
    "        \"conversation_id\": conversation_id,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model_key\": model_key,\n",
    "        \"model_id\": model_id,\n",
    "        \"tokens_used\": tokens\n",
    "    }\n",
    "    \n",
    "    # Log metadata to MLflow\n",
    "    mlflow.log_param(\"conversation_id\", conversation_id)\n",
    "    mlflow.log_param(\"model_key\", model_key)\n",
    "    mlflow.log_param(\"model_id\", model_id)\n",
    "    mlflow.log_metric(\"tokens_used\", tokens)\n",
    "    \n",
    "    return response\n",
    "\n",
    "print(\"‚úì Groq-powered chat functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa930b55",
   "metadata": {},
   "source": [
    "## Step 3: Generate Sample Conversations (A/B Testing)\n",
    "\n",
    "Let's simulate user-LLM interactions using different models for A/B testing.\n",
    "We'll test multiple Groq models to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a00603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample questions from config\n",
    "sample_questions = config['sample_questions']\n",
    "\n",
    "# For A/B testing: load test models from config\n",
    "ab_testing_enabled = config['ab_testing']['enabled']\n",
    "test_models = config['ab_testing']['test_models'] if ab_testing_enabled else [config['groq']['default_model']]\n",
    "\n",
    "# Generate conversations and capture traces\n",
    "print(\"Generating chat interactions with A/B testing...\\n\")\n",
    "print(f\"Model A: {GROQ_MODELS[test_models[0]]}\")\n",
    "print(f\"Model B: {GROQ_MODELS[test_models[1]]}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "conversations = []\n",
    "for idx, question in enumerate(sample_questions):\n",
    "    # Alternate between models for A/B testing\n",
    "    model_key = test_models[idx % len(test_models)]\n",
    "    conversation_id = f\"conv_{idx+1}\"\n",
    "    \n",
    "    # Start an MLflow run for this conversation\n",
    "    with mlflow.start_run(run_name=f\"chat_{idx+1}_{model_key}\"):\n",
    "        # Chat with LLM (this creates a trace)\n",
    "        response = chat_with_llm(question, conversation_id, model_key)\n",
    "        conversations.append(response)\n",
    "        \n",
    "        print(f\"[{conversation_id}] Model: {model_key}\")\n",
    "        print(f\"Q: {question}\")\n",
    "        print(f\"A: {response['answer'][:150]}...\" if len(response['answer']) > 150 else f\"A: {response['answer']}\")\n",
    "        print(f\"Tokens: {response['tokens_used']}\\n\")\n",
    "\n",
    "print(f\"‚úì Generated {len(conversations)} conversations with traces\")\n",
    "print(f\"‚úì Used {len(test_models)} different models for A/B testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59ebdce",
   "metadata": {},
   "source": [
    "## Step 4: Retrieve and Inspect Traces\n",
    "\n",
    "Now let's retrieve the traces we just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f5a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for traces in our experiment\n",
    "traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"attributes.name = 'chat_completion'\",\n",
    "    max_results=100,\n",
    "    return_type=\"list\"  # Returns list[Trace] for direct manipulation\n",
    ")\n",
    "\n",
    "print(f\"Found {len(traces)} traces\\n\")\n",
    "\n",
    "# Inspect the first trace\n",
    "if traces:\n",
    "    first_trace = traces[0]\n",
    "    print(\"Sample Trace Structure:\")\n",
    "    print(f\"  Trace ID: {first_trace.info.trace_id}\")\n",
    "    print(f\"  Trace Name: {first_trace.info.trace_name}\")\n",
    "    print(f\"  Execution Time: {first_trace.info.execution_time_ms}ms\")\n",
    "    print(f\"  Status: {first_trace.info.status}\")\n",
    "    print(f\"\\n  Data Keys: {first_trace.data.keys() if hasattr(first_trace.data, 'keys') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698dbb49",
   "metadata": {},
   "source": [
    "                \"## Step 5: Add Expectations (Ground Truth Annotations)\",\n",
    "                \"\",\n",
    "                \"**Expectations** are the ground truth against which we evaluate the LLM's outputs.\",\n",
    "                \"They can be:\",\n",
    "                \"- Specific expected text/answers (reference responses)\",\n",
    "                \"- Quality metrics (relevance, accuracy scores)\",\n",
    "                \"- Boolean flags (contains_citation, is_helpful, etc.)\",\n",
    "                \"- Structured evaluation criteria\",\n",
    "                \"\",\n",
    "                \"**Configuration:**\",\n",
    "                \"- Expected answers are defined in `config.yaml` under `expected_answers`\",\n",
    "                \"- Each answer includes the question, expected response, and quality metrics\",\n",
    "                \"- **To add/modify expectations, edit config.yaml!**\",\n",
    "                \"\",\n",
    "                \"This approach allows:\",\n",
    "                \"- Easy updates without code changes\",\n",
    "                \"- Version control of test data\",\n",
    "                \"- Team collaboration on ground truth\",\n",
    "                \"- Centralized test case management\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b784aacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_expectations_to_trace(trace, expected_answer: str, quality_metrics: dict):\n",
    "    \"\"\"\n",
    "    Add expectations (ground truth) to a trace for evaluation.\n",
    "    \n",
    "    Args:\n",
    "        trace: MLflow Trace object\n",
    "        expected_answer: The expected/ideal answer\n",
    "        quality_metrics: Dictionary of quality scores and criteria\n",
    "    \"\"\"\n",
    "    trace_id = trace.info.trace_id\n",
    "    \n",
    "    # Log expected answer\n",
    "    mlflow.log_expectation(\n",
    "        trace_id=trace_id,\n",
    "        name=\"expected_answer\",\n",
    "        value=expected_answer\n",
    "    )\n",
    "    \n",
    "    # Log quality metrics as structured expectations\n",
    "    mlflow.log_expectation(\n",
    "        trace_id=trace_id,\n",
    "        name=\"quality_metrics\",\n",
    "        value=quality_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "# Load expectations (reference answers and quality metrics) from config\n",
    "# In a real scenario, these would come from expert review, human annotation, or ground truth data\n",
    "# Now centrally managed in config.yaml for easy updates\n",
    "expectations_map = [\n",
    "    {\n",
    "        \"expected_answer\": exp['answer'],\n",
    "        \"quality_metrics\": exp['quality_metrics']\n",
    "    }\n",
    "    for exp in config['expected_answers']\n",
    "]\n",
    "\n",
    "# Add expectations to traces\n",
    "print(\"Adding expectations to traces...\\n\")\n",
    "\n",
    "for idx, trace in enumerate(traces[:len(expectations_map)]):\n",
    "    expectations = expectations_map[idx]\n",
    "    add_expectations_to_trace(\n",
    "        trace, \n",
    "        expectations[\"expected_answer\"],\n",
    "        expectations[\"quality_metrics\"]\n",
    "    )\n",
    "    print(f\"‚úì Added expectations to trace {idx+1}\")\n",
    "\n",
    "print(f\"\\n‚úì Added expectations to {len(expectations_map)} traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9556958",
   "metadata": {},
   "source": [
    "## Step 5: Retrieve Annotated Traces\n",
    "\n",
    "Now let's retrieve the traces with their expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f617fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve traces with expectations\n",
    "annotated_traces = mlflow.search_traces(\n",
    "    experiment_ids=[experiment_id],\n",
    "    filter_string=\"attributes.name = 'chat_completion'\",\n",
    "    max_results=100,\n",
    "    return_type=\"list\"\n",
    ")\n",
    "\n",
    "print(f\"Retrieved {len(annotated_traces)} annotated traces\\n\")\n",
    "\n",
    "# Display a sample annotated trace\n",
    "if annotated_traces:\n",
    "    sample_trace = annotated_traces[0]\n",
    "    print(\"Sample Annotated Trace:\")\n",
    "    print(f\"  Trace ID: {sample_trace.info.trace_id}\")\n",
    "    \n",
    "    # Note: Expectations are stored separately and can be retrieved\n",
    "    # They will be included when we build the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f276885",
   "metadata": {},
   "source": [
    "## Step 6: Build Evaluation Dataset from Traces\n",
    "\n",
    "Now we create an evaluation dataset by merging our annotated traces.\n",
    "This dataset becomes a reusable test suite for systematic evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation dataset using config\n",
    "dataset_name = config['dataset']['name']\n",
    "dataset_tags = config['dataset']['tags'].copy()\n",
    "dataset_tags['created_date'] = datetime.now().isoformat()\n",
    "dataset_tags['models_tested'] = ','.join(test_models)\n",
    "\n",
    "print(f\"Creating evaluation dataset: {dataset_name}...\\n\")\n",
    "\n",
    "dataset = create_dataset(\n",
    "    name=dataset_name,\n",
    "    experiment_id=[experiment_id],\n",
    "    tags=dataset_tags\n",
    ")\n",
    "\n",
    "# Merge annotated traces into the dataset\n",
    "# Use only traces that have expectations added\n",
    "traces_with_expectations = annotated_traces[:len(expectations_map)]\n",
    "dataset.merge_records(traces_with_expectations)\n",
    "\n",
    "print(f\"‚úì Created dataset '{dataset_name}'\")\n",
    "print(f\"‚úì Merged {len(traces_with_expectations)} traces into dataset\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"  Name: {dataset.name}\")\n",
    "print(f\"  Tags: {dataset.tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4550f8d",
   "metadata": {},
   "source": [
    "## Step 7: Load and Inspect the Dataset\n",
    "\n",
    "Let's load our dataset and see what it contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419894f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "loaded_dataset = mlflow.genai.datasets.load_dataset(dataset_name)\n",
    "\n",
    "print(f\"Dataset: {loaded_dataset.name}\\n\")\n",
    "\n",
    "# Convert to pandas DataFrame for easier inspection\n",
    "try:\n",
    "    df = loaded_dataset.to_pandas()\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"\\nColumns: {df.columns.tolist()}\\n\")\n",
    "    print(\"Sample records:\")\n",
    "    print(df.head())\n",
    "except Exception as e:\n",
    "    print(f\"Note: {e}\")\n",
    "    print(\"Dataset structure may vary - inspect using dataset API methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2529f8c7",
   "metadata": {},
   "source": [
    "## Step 8: Advanced Evaluation Metrics\n",
    "\n",
    "We'll evaluate LLM responses using industry-standard metrics:\n",
    "\n",
    "### Metrics Implemented:\n",
    "1. **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)**\n",
    "   - ROUGE-1: Unigram overlap\n",
    "   - ROUGE-2: Bigram overlap\n",
    "   - ROUGE-L: Longest common subsequence\n",
    "   - Common in summarization and text generation\n",
    "\n",
    "2. **BLEU (Bilingual Evaluation Understudy)**\n",
    "   - Measures n-gram precision\n",
    "   - Originally for machine translation, now widely used in NLG\n",
    "\n",
    "3. **METEOR (Metric for Evaluation of Translation with Explicit ORdering)**\n",
    "   - Considers synonyms and stemming\n",
    "   - More sophisticated than BLEU\n",
    "\n",
    "4. **Custom Metrics**\n",
    "   - Semantic similarity (Jaccard)\n",
    "   - Exact match\n",
    "   - Length appropriateness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5bbec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluation scorers using config\n",
    "rouge_config = config['evaluation']['metrics']['rouge']\n",
    "rouge = rouge_scorer.RougeScorer(\n",
    "    rouge_config['types'], \n",
    "    use_stemmer=rouge_config['use_stemmer']\n",
    ") if rouge_config['enabled'] else None\n",
    "\n",
    "bleu_config = config['evaluation']['metrics']['bleu']\n",
    "smoothing = SmoothingFunction().method1 if bleu_config.get('smoothing', True) else None\n",
    "\n",
    "\n",
    "def calculate_rouge_scores(prediction: str, reference: str) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate ROUGE scores (ROUGE-1, ROUGE-2, ROUGE-L).\n",
    "    \n",
    "    ROUGE measures recall-oriented overlap between prediction and reference.\n",
    "    Higher scores indicate better overlap with the reference text.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with ROUGE-1, ROUGE-2, and ROUGE-L F1 scores\n",
    "    \"\"\"\n",
    "    scores = rouge.score(reference, prediction)\n",
    "    return {\n",
    "        'rouge1': scores['rouge1'].fmeasure,\n",
    "        'rouge2': scores['rouge2'].fmeasure,\n",
    "        'rougeL': scores['rougeL'].fmeasure\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_bleu_score(prediction: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate BLEU score.\n",
    "    \n",
    "    BLEU measures precision of n-grams in prediction against reference.\n",
    "    Score ranges from 0 to 1, where 1 is perfect match.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        BLEU score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    reference_tokens = [reference.lower().split()]\n",
    "    prediction_tokens = prediction.lower().split()\n",
    "    \n",
    "    try:\n",
    "        score = sentence_bleu(\n",
    "            reference_tokens, \n",
    "            prediction_tokens, \n",
    "            smoothing_function=smoothing\n",
    "        )\n",
    "        return score\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_meteor_score(prediction: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate METEOR score.\n",
    "    \n",
    "    METEOR considers synonyms, stemming, and word order.\n",
    "    More sophisticated than BLEU for semantic similarity.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        METEOR score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize\n",
    "        reference_tokens = reference.lower().split()\n",
    "        prediction_tokens = prediction.lower().split()\n",
    "        score = meteor_score([reference_tokens], prediction_tokens)\n",
    "        return score\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def calculate_semantic_similarity(prediction: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Jaccard similarity (word-level overlap).\n",
    "    \n",
    "    Simple but effective measure of lexical similarity.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        Jaccard similarity (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    pred_words = set(prediction.lower().split())\n",
    "    ref_words = set(reference.lower().split())\n",
    "    \n",
    "    if not pred_words or not ref_words:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = pred_words.intersection(ref_words)\n",
    "    union = pred_words.union(ref_words)\n",
    "    \n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "\n",
    "def exact_match_score(prediction: str, reference: str) -> float:\n",
    "    \"\"\"\n",
    "    Check for exact match between prediction and reference.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        1.0 if exact match, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    return 1.0 if prediction.strip().lower() == reference.strip().lower() else 0.0\n",
    "\n",
    "\n",
    "def evaluate_response(prediction: str, reference: str) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation using all metrics.\n",
    "    \n",
    "    Args:\n",
    "        prediction: Generated text from LLM\n",
    "        reference: Ground truth/expected text\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with all evaluation scores\n",
    "    \"\"\"\n",
    "    rouge_scores = calculate_rouge_scores(prediction, reference)\n",
    "    bleu = calculate_bleu_score(prediction, reference)\n",
    "    meteor = calculate_meteor_score(prediction, reference)\n",
    "    semantic_sim = calculate_semantic_similarity(prediction, reference)\n",
    "    exact_match = exact_match_score(prediction, reference)\n",
    "    \n",
    "    return {\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'bleu': bleu,\n",
    "        'meteor': meteor,\n",
    "        'semantic_similarity': semantic_sim,\n",
    "        'exact_match': exact_match,\n",
    "        'response_length': len(prediction)\n",
    "    }\n",
    "\n",
    "print(\"‚úì Advanced evaluation metrics defined\")\n",
    "print(\"\\nMetrics available:\")\n",
    "print(\"  - ROUGE-1, ROUGE-2, ROUGE-L\")\n",
    "print(\"  - BLEU\")\n",
    "print(\"  - METEOR\")\n",
    "print(\"  - Semantic Similarity (Jaccard)\")\n",
    "print(\"  - Exact Match\")\n",
    "print(\"  - Response Length\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0f79e8",
   "metadata": {},
   "source": [
    "## Step 9: Run Comprehensive Evaluation\n",
    "\n",
    "Let's evaluate all conversations using our advanced metrics and compare models (A/B testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a61fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each conversation\n",
    "print(\"Running comprehensive evaluation...\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for idx, conversation in enumerate(conversations[:len(expectations_map)]):\n",
    "    expected = expectations_map[idx]\n",
    "    \n",
    "    # Get prediction and reference\n",
    "    prediction = conversation['answer']\n",
    "    reference = expected['expected_answer']\n",
    "    \n",
    "    # Calculate all evaluation metrics\n",
    "    scores = evaluate_response(prediction, reference)\n",
    "    \n",
    "    # Add metadata\n",
    "    result = {\n",
    "        'conversation_id': conversation['conversation_id'],\n",
    "        'question': conversation['question'],\n",
    "        'model_key': conversation['model_key'],\n",
    "        'model_id': conversation['model_id'],\n",
    "        'tokens_used': conversation['tokens_used'],\n",
    "        'prediction': prediction,\n",
    "        'reference': reference,\n",
    "        **scores  # Unpack all metric scores\n",
    "    }\n",
    "    \n",
    "    evaluation_results.append(result)\n",
    "    \n",
    "    # Print summary for this conversation\n",
    "    print(f\"\\nConversation {idx+1} [{conversation['model_key']}]:\")\n",
    "    print(f\"Q: {conversation['question']}\")\n",
    "    print(f\"\\nMetrics:\")\n",
    "    print(f\"  ROUGE-1: {scores['rouge1']:.3f}\")\n",
    "    print(f\"  ROUGE-2: {scores['rouge2']:.3f}\")\n",
    "    print(f\"  ROUGE-L: {scores['rougeL']:.3f}\")\n",
    "    print(f\"  BLEU:    {scores['bleu']:.3f}\")\n",
    "    print(f\"  METEOR:  {scores['meteor']:.3f}\")\n",
    "    print(f\"  Semantic Similarity: {scores['semantic_similarity']:.3f}\")\n",
    "    print(f\"  Tokens Used: {conversation['tokens_used']}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "results_df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "print(f\"\\n‚úì Evaluated {len(evaluation_results)} conversations\")\n",
    "print(f\"\\nResults DataFrame Shape: {results_df.shape}\")\n",
    "print(f\"Columns: {results_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebc1b3e",
   "metadata": {},
   "source": [
    "## Step 10: Statistical Analysis & A/B Testing Comparison\n",
    "\n",
    "Analyze evaluation results and compare model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18147868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate aggregate statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Metric columns for analysis\n",
    "metric_cols = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'meteor', 'semantic_similarity']\n",
    "\n",
    "print(\"\\nüìä Overall Performance Metrics:\")\n",
    "print(\"-\" * 80)\n",
    "for metric in metric_cols:\n",
    "    mean_val = results_df[metric].mean()\n",
    "    std_val = results_df[metric].std()\n",
    "    min_val = results_df[metric].min()\n",
    "    max_val = results_df[metric].max()\n",
    "    print(f\"{metric.upper():20s} | Mean: {mean_val:.3f} | Std: {std_val:.3f} | Min: {min_val:.3f} | Max: {max_val:.3f}\")\n",
    "\n",
    "# A/B Testing Analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"A/B TESTING: MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Group by model\n",
    "model_comparison = results_df.groupby('model_key')[metric_cols].agg(['mean', 'std'])\n",
    "\n",
    "print(\"\\nüìà Performance by Model:\")\n",
    "print(model_comparison.round(3))\n",
    "\n",
    "# Determine winner for each metric\n",
    "print(\"\\nüèÜ A/B Test Winners (by metric):\")\n",
    "print(\"-\" * 80)\n",
    "for metric in metric_cols:\n",
    "    model_means = results_df.groupby('model_key')[metric].mean()\n",
    "    winner = model_means.idxmax()\n",
    "    winner_score = model_means.max()\n",
    "    print(f\"{metric.upper():20s} | Winner: {winner:15s} | Score: {winner_score:.3f}\")\n",
    "\n",
    "# Token efficiency comparison\n",
    "print(\"\\nüí∞ Token Efficiency:\")\n",
    "print(\"-\" * 80)\n",
    "token_stats = results_df.groupby('model_key')['tokens_used'].agg(['mean', 'sum', 'min', 'max'])\n",
    "print(token_stats.round(0))\n",
    "\n",
    "# Overall recommendation\n",
    "print(\"\\nüí° Recommendation:\")\n",
    "print(\"-\" * 80)\n",
    "avg_performance = results_df.groupby('model_key')[metric_cols].mean().mean(axis=1)\n",
    "best_model = avg_performance.idxmax()\n",
    "best_score = avg_performance.max()\n",
    "print(f\"Best Overall Model: {best_model}\")\n",
    "print(f\"Average Score: {best_score:.3f}\")\n",
    "print(f\"\\nAll model averages:\")\n",
    "for model, score in avg_performance.items():\n",
    "    print(f\"  {model}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb515fe",
   "metadata": {},
   "source": [
    "## Step 11: Visualization of Evaluation Results\n",
    "\n",
    "Create comprehensive visualizations to understand model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822931d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Metric Comparison Across Models (Bar Chart)\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('LLM Evaluation Metrics: Model Comparison (A/B Testing)', fontsize=16, fontweight='bold')\n",
    "\n",
    "metrics_to_plot = ['rouge1', 'rouge2', 'rougeL', 'bleu', 'meteor', 'semantic_similarity']\n",
    "titles = ['ROUGE-1 Score', 'ROUGE-2 Score', 'ROUGE-L Score', 'BLEU Score', 'METEOR Score', 'Semantic Similarity']\n",
    "\n",
    "for idx, (metric, title) in enumerate(zip(metrics_to_plot, titles)):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    \n",
    "    # Group data by model\n",
    "    model_data = results_df.groupby('model_key')[metric].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Create bar plot\n",
    "    bars = ax.bar(model_data['model_key'], model_data['mean'], \n",
    "                   yerr=model_data['std'], capsize=5, alpha=0.7,\n",
    "                   color=['#3498db', '#e74c3c', '#2ecc71'][:len(model_data)])\n",
    "    \n",
    "    ax.set_title(title, fontweight='bold')\n",
    "    ax.set_ylabel('Score', fontsize=10)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=15)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('evaluation_metrics_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: evaluation_metrics_comparison.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4266a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Radar Chart for Overall Model Comparison\n",
    "from math import pi\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "# Prepare data\n",
    "categories = ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', 'METEOR', 'Semantic Sim']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angle for each axis\n",
    "angles = [n / float(num_vars) * 2 * pi for n in range(num_vars)]\n",
    "angles += angles[:1]\n",
    "\n",
    "# Plot for each model\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#f39c12']\n",
    "for idx, model in enumerate(results_df['model_key'].unique()):\n",
    "    model_data = results_df[results_df['model_key'] == model]\n",
    "    values = [\n",
    "        model_data['rouge1'].mean(),\n",
    "        model_data['rouge2'].mean(),\n",
    "        model_data['rougeL'].mean(),\n",
    "        model_data['bleu'].mean(),\n",
    "        model_data['meteor'].mean(),\n",
    "        model_data['semantic_similarity'].mean()\n",
    "    ]\n",
    "    values += values[:1]  # Complete the circle\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors[idx % len(colors)])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors[idx % len(colors)])\n",
    "\n",
    "# Customize plot\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories, size=11)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'], size=9)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=11)\n",
    "ax.set_title('Model Performance Radar Chart\\n(A/B Testing Comparison)', \n",
    "            size=14, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_performance_radar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: model_performance_radar.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c534e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Heatmap of Metric Correlations\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = results_df[metric_cols].corr()\n",
    "\n",
    "# Create heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "            center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            ax=ax)\n",
    "\n",
    "ax.set_title('Correlation Between Evaluation Metrics', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: metrics_correlation_heatmap.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8241a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Token Usage vs Performance Scatter Plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Calculate average performance score\n",
    "results_df['avg_score'] = results_df[metric_cols].mean(axis=1)\n",
    "\n",
    "# Create scatter plot for each model\n",
    "for idx, model in enumerate(results_df['model_key'].unique()):\n",
    "    model_data = results_df[results_df['model_key'] == model]\n",
    "    ax.scatter(model_data['tokens_used'], model_data['avg_score'], \n",
    "              s=150, alpha=0.7, label=model, color=colors[idx % len(colors)])\n",
    "\n",
    "ax.set_xlabel('Tokens Used', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Average Performance Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Token Efficiency vs Performance\\n(Lower tokens + Higher score = Better)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('token_efficiency_vs_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: token_efficiency_vs_performance.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61b67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Box Plot for Metric Distribution by Model\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Prepare data in long format\n",
    "plot_data = []\n",
    "for model in results_df['model_key'].unique():\n",
    "    model_data = results_df[results_df['model_key'] == model]\n",
    "    for metric in metric_cols:\n",
    "        for value in model_data[metric]:\n",
    "            plot_data.append({\n",
    "                'Model': model,\n",
    "                'Metric': metric.upper(),\n",
    "                'Score': value\n",
    "            })\n",
    "\n",
    "plot_df = pd.DataFrame(plot_data)\n",
    "\n",
    "# Create box plot\n",
    "sns.boxplot(data=plot_df, x='Metric', y='Score', hue='Model', ax=ax, palette='Set2')\n",
    "\n",
    "ax.set_title('Distribution of Evaluation Metrics by Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Metric', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.legend(title='Model', fontsize=10)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_distribution_boxplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Saved: metrics_distribution_boxplot.png\")\n",
    "\n",
    "print(\"\\n‚úÖ All visualizations created successfully!\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. evaluation_metrics_comparison.png\")\n",
    "print(\"  2. model_performance_radar.png\")\n",
    "print(\"  3. metrics_correlation_heatmap.png\")\n",
    "print(\"  4. token_efficiency_vs_performance.png\")\n",
    "print(\"  5. metrics_distribution_boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727d75be",
   "metadata": {},
   "source": [
    "## Step 12: Continuous Evaluation Loop\n",
    "\n",
    "Demonstrate how to continuously add new conversations and update the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f3d4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def continuous_evaluation_workflow(new_question: str, conversation_id: str):\n",
    "    \"\"\"\n",
    "    Demonstrate the continuous evaluation loop:\n",
    "    1. User asks question\n",
    "    2. LLM responds (trace captured)\n",
    "    3. Add expectations\n",
    "    4. Update dataset\n",
    "    5. Re-evaluate\n",
    "    \n",
    "    Args:\n",
    "        new_question: New user question\n",
    "        conversation_id: Unique conversation identifier\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CONTINUOUS EVALUATION WORKFLOW\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Step 1: User interaction\n",
    "    print(f\"1Ô∏è‚É£  User Question: {new_question}\")\n",
    "    \n",
    "    # Step 2: Generate response and capture trace\n",
    "    with mlflow.start_run(run_name=f\"continuous_eval_{conversation_id}\"):\n",
    "        response = chat_with_llm(new_question, conversation_id)\n",
    "        print(f\"2Ô∏è‚É£  LLM Response: {response['answer']}\")\n",
    "        print(f\"   ‚úì Trace captured\")\n",
    "    \n",
    "    # Step 3: Add expectations (in real scenario, this could be async/manual)\n",
    "    new_traces = mlflow.search_traces(\n",
    "        experiment_ids=[experiment_id],\n",
    "        filter_string=f\"attributes.name = 'chat_completion'\",\n",
    "        max_results=1,\n",
    "        return_type=\"list\",\n",
    "        order_by=[\"timestamp DESC\"]\n",
    "    )\n",
    "    \n",
    "    if new_traces:\n",
    "        new_trace = new_traces[0]\n",
    "        # Simulate adding expectations\n",
    "        add_expectations_to_trace(\n",
    "            new_trace,\n",
    "            expected_answer=\"Sample expected answer for new question\",\n",
    "            quality_metrics={\n",
    "                \"relevance\": 0.9,\n",
    "                \"accuracy\": 0.85,\n",
    "                \"completeness\": 0.9\n",
    "            }\n",
    "        )\n",
    "        print(f\"3Ô∏è‚É£  Expectations added to trace\")\n",
    "        \n",
    "        # Step 4: Update dataset\n",
    "        dataset.merge_records([new_trace])\n",
    "        print(f\"4Ô∏è‚É£  Dataset updated with new trace\")\n",
    "    \n",
    "    # Step 5: Ready for evaluation\n",
    "    print(f\"5Ô∏è‚É£  Ready for re-evaluation with updated dataset\")\n",
    "    print(f\"\\n‚úì Continuous evaluation workflow complete\\n\")\n",
    "\n",
    "\n",
    "# Example: Add a new conversation\n",
    "continuous_evaluation_workflow(\n",
    "    \"What is the capital of Spain?\",\n",
    "    \"conv_new_1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b098a92",
   "metadata": {},
   "source": [
    "## Step 12: View Evaluation in MLflow UI\n",
    "\n",
    "You can view all traces, datasets, and evaluations in the MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46fef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VIEWING RESULTS IN MLFLOW UI\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nTo view your evaluation results in MLflow UI:\")\n",
    "print(\"\\n1. Run in terminal:\")\n",
    "print(\"   mlflow ui --port 5000\")\n",
    "print(\"\\n2. Open browser and navigate to:\")\n",
    "print(\"   http://localhost:5000\")\n",
    "print(\"\\n3. Explore:\")\n",
    "print(\"   - Experiments ‚Üí \", experiment_name)\n",
    "print(\"   - Traces tab to view all chat interactions\")\n",
    "print(\"   - Datasets tab to view evaluation datasets\")\n",
    "print(\"   - Compare runs to see performance trends\")\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e4ed7d",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We Accomplished:\n",
    "\n",
    "1. ‚úÖ **Groq API Integration**: Used fast, production-ready LLM inference\n",
    "2. ‚úÖ **Real Chat Interactions**: Created user-LLM conversations with multiple models\n",
    "3. ‚úÖ **MLflow Trace Capture**: Recorded each interaction automatically\n",
    "4. ‚úÖ **Ground Truth Annotations**: Added expectations (reference answers)\n",
    "5. ‚úÖ **Dataset from Traces**: Built reusable evaluation dataset\n",
    "6. ‚úÖ **Advanced Metrics**: Implemented ROUGE, BLEU, METEOR, Semantic Similarity\n",
    "7. ‚úÖ **A/B Testing**: Compared different Groq models systematically\n",
    "8. ‚úÖ **Comprehensive Visualizations**: 5 different charts for analysis\n",
    "9. ‚úÖ **Token Efficiency Analysis**: Cost-benefit evaluation\n",
    "10. ‚úÖ **Continuous Evaluation Loop**: Showed iterative improvement workflow\n",
    "\n",
    "### Why This Approach Works:\n",
    "\n",
    "- **Real-World Grounded**: Evaluation data from actual Groq API interactions\n",
    "- **Industry-Standard Metrics**: ROUGE, BLEU, METEOR used in production systems\n",
    "- **Data-Driven A/B Testing**: Compare models objectively with multiple metrics\n",
    "- **Continuous Improvement**: Dataset evolves with each conversation\n",
    "- **Cost Optimization**: Track token usage and efficiency\n",
    "- **Visual Insights**: Easy-to-understand charts for stakeholders\n",
    "- **Version Tracking**: Compare models consistently over time\n",
    "- **MLflow Integration**: Enterprise-ready tracking and deployment\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Scale Up**: Test with larger datasets (100+ conversations)\n",
    "2. **More Models**: Add other Groq models (Mixtral, Gemma) for comparison\n",
    "3. **Temperature Testing**: A/B test different temperature settings\n",
    "4. **Custom Metrics**: Add domain-specific evaluation metrics\n",
    "5. **Human Evaluation**: Integrate human feedback loop\n",
    "6. **Production Pipeline**: Automate trace collection from live traffic\n",
    "7. **Alerting**: Set up metric degradation alerts\n",
    "8. **CI/CD Integration**: Automated testing before deployment\n",
    "9. **Cost Analysis**: Track API costs vs performance\n",
    "10. **Model Fine-tuning**: Use insights to improve model selection\n",
    "\n",
    "### Resources:\n",
    "\n",
    "- [MLflow GenAI Datasets](https://mlflow.org/docs/latest/genai/datasets/) - Dataset building from traces\n",
    "- [MLflow Tracing Guide](https://mlflow.org/docs/latest/genai/tracing/) - Capture LLM interactions\n",
    "- [MLflow Evaluation Framework](https://mlflow.org/docs/latest/genai/eval-monitor/) - Systematic evaluation\n",
    "- [Groq API Documentation](https://console.groq.com/docs) - Fast LLM inference\n",
    "- [ROUGE Score Guide](https://aclanthology.org/W04-1013/) - Understanding ROUGE metrics\n",
    "- [BLEU Score Paper](https://aclanthology.org/P02-1040/) - BLEU metric details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3787ef8",
   "metadata": {},
   "source": [
    "## Additional: A/B Testing Strategies with Groq\n",
    "\n",
    "### A/B Testing Approaches:\n",
    "\n",
    "1. **Model Comparison** (implemented above):\n",
    "   - Test different models (llama-3.1-8b vs llama-3.1-70b)\n",
    "   - Compare performance, cost, and speed\n",
    "\n",
    "2. **Temperature Testing**:\n",
    "   - Same model, different temperatures\n",
    "   - Evaluate creativity vs consistency\n",
    "\n",
    "3. **Prompt Engineering**:\n",
    "   - Test different system prompts\n",
    "   - Compare instruction formats\n",
    "\n",
    "4. **Context Window Testing**:\n",
    "   - Test with varying context lengths\n",
    "   - Evaluate performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: A/B Test with Different Temperatures\n",
    "\n",
    "def ab_test_temperatures():\n",
    "    \"\"\"\n",
    "    Example of A/B testing with different temperature settings.\n",
    "    Higher temperature = more creative/random\n",
    "    Lower temperature = more focused/deterministic\n",
    "    \"\"\"\n",
    "    temperatures = [0.3, 0.7, 1.0]  # Test different creativity levels\n",
    "    test_question = \"Explain artificial intelligence\"\n",
    "    \n",
    "    results = []\n",
    "    for temp in temperatures:\n",
    "        response, model_id, tokens = groq_llm_response(\n",
    "            test_question, \n",
    "            model_key=\"llama-3.1-8b\",\n",
    "            temperature=temp\n",
    "        )\n",
    "        results.append({\n",
    "            'temperature': temp,\n",
    "            'response': response,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        print(response[:200] + \"...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run temperature A/B test\n",
    "# temp_results = ab_test_temperatures()\n",
    "\n",
    "print(\"‚úì A/B testing examples defined\")\n",
    "print(\"\\nTo run temperature test, uncomment: temp_results = ab_test_temperatures()\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
