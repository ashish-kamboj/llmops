{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf05c66",
   "metadata": {},
   "source": [
    "# Prompt Versioning and LLM Evaluation with Groq API\n",
    "\n",
    "This comprehensive notebook demonstrates how to systematically evaluate and version prompt templates using MLflow's GenAI evaluation framework with Groq API for LLM inference. Learn how to build a robust prompt engineering workflow with quantitative evaluation metrics.\n",
    "\n",
    "## ğŸ¯ What You'll Learn\n",
    "\n",
    "This tutorial covers the complete prompt engineering lifecycle:\n",
    "\n",
    "1. **Setup & Configuration** - MLflow tracking and Groq API integration\n",
    "2. **Prompt Versioning** - Creating and managing multiple prompt versions\n",
    "3. **Evaluation Datasets** - Building comprehensive test cases\n",
    "4. **LLM Integration** - Using Groq API for fast, cost-effective inference\n",
    "5. **Custom Scorers** - Building both LLM-based and heuristic evaluation metrics\n",
    "6. **Systematic Evaluation** - Running evaluations and analyzing results\n",
    "7. **Iterative Improvement** - Data-driven prompt optimization workflow\n",
    "\n",
    "## ğŸš€ Key Benefits\n",
    "\n",
    "- **Cost-Effective**: Uses Groq's fast inference instead of expensive OpenAI API calls\n",
    "- **Version Control**: Track prompt changes and their performance impact\n",
    "- **Quantitative Metrics**: Measure improvements with concrete evaluation scores\n",
    "- **Professional Workflow**: Enterprise-grade prompt engineering practices\n",
    "- **Reproducible**: All experiments tracked in MLflow for easy comparison\n",
    "\n",
    "## ğŸ“‹ Prerequisites\n",
    "\n",
    "### Required Packages\n",
    "```bash\n",
    "pip install --upgrade mlflow>=3.3 groq\n",
    "```\n",
    "\n",
    "### API Keys Required\n",
    "- **Groq API Key**: Get one from [https://console.groq.com/keys](https://console.groq.com/keys)\n",
    "- Set your API key as an environment variable:\n",
    "  ```bash\n",
    "  export GROQ_API_KEY=\"your-groq-api-key-here\"\n",
    "  ```\n",
    "\n",
    "### MLflow Server\n",
    "Start the MLflow tracking server:\n",
    "```bash\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5000\n",
    "```\n",
    "\n",
    "## ğŸ”§ System Requirements\n",
    "\n",
    "- Python 3.8+\n",
    "- MLflow 3.3+\n",
    "- Groq API access\n",
    "- Internet connection for API calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b677c8",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Setup and Configuration\n",
    "\n",
    "In this step, we'll import the necessary libraries and configure our MLflow tracking server and Groq API client. This foundation enables us to track experiments and make LLM calls efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb92d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for MLflow tracking and Groq API integration\n",
    "import os\n",
    "import json\n",
    "import mlflow\n",
    "from groq import Groq\n",
    "from mlflow.entities import Feedback\n",
    "from mlflow.genai import scorer\n",
    "from mlflow.genai.scorers import Guidelines\n",
    "\n",
    "# Configure MLflow tracking server\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"llm-evaluation\")\n",
    "\n",
    "# Retrieve and validate Groq API key\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\", \"your-groq-api-key-here\")\n",
    "\n",
    "if GROQ_API_KEY == \"your-groq-api-key-here\":\n",
    "    print(\"âš ï¸  Please set your GROQ_API_KEY environment variable or update the key in this cell\")\n",
    "    print(\"Get your API key from: https://console.groq.com/keys\")\n",
    "    raise ValueError(\"Groq API key not configured. Please set GROQ_API_KEY environment variable.\")\n",
    "else:\n",
    "    print(\"âœ… Groq API key configured successfully\")\n",
    "\n",
    "# Initialize Groq client for LLM inference\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Display configuration status\n",
    "print(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
    "print(f\"Current experiment: llm-evaluation\")\n",
    "print(\"ğŸš€ Setup complete! Ready to start prompt engineering workflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1a3fc4",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 2: Create Prompt Templates\n",
    "\n",
    "Now we'll define and version prompt templates for our evaluation. This demonstrates the iterative improvement process where we can track changes and measure their impact on performance.\n",
    "\n",
    "### ğŸ”„ Prompt Version 1: Basic Q&A Template\n",
    "\n",
    "Our first version is a simple, straightforward Q&A template without specific formatting instructions. This serves as our baseline for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1830194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the first version of our prompt template\n",
    "# This is a basic Q&A template without specific formatting instructions\n",
    "# It serves as our baseline for comparison with improved versions\n",
    "PROMPT_V1 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. Answer the following question.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        # Use double curly braces {{}} to indicate template variables\n",
    "        # These will be filled in during evaluation with actual questions\n",
    "        \"content\": \"Question: {{question}}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Register the prompt template in MLflow Prompt Registry for version control\n",
    "# This enables tracking changes, loading specific versions, and comparing performance\n",
    "mlflow.genai.register_prompt(\n",
    "    name=\"qa_prompt\",\n",
    "    template=PROMPT_V1,\n",
    "    commit_message=\"Initial basic Q&A prompt - baseline version\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt V1 registered successfully!\")\n",
    "print(\"\\nğŸ“‹ Prompt V1 structure:\")\n",
    "for i, msg in enumerate(PROMPT_V1):\n",
    "    print(f\"  {i+1}. {msg['role'].title()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ This version will serve as our baseline for measuring improvements.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dc5d59",
   "metadata": {},
   "source": [
    "### âš¡ Prompt Version 2: Enhanced Template with Formatting Instructions\n",
    "\n",
    "Building on our baseline, this version adds specific formatting instructions to encourage more concise and professional responses. We'll measure how these changes impact our evaluation metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68fa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an improved version of our prompt template\n",
    "# This version includes specific formatting instructions for better response quality\n",
    "# Key improvements: length constraints and professional tone requirements\n",
    "PROMPT_V2 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. Answer the following question in three sentences or less. Be concise and professional.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: {{question}}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Register the improved prompt as a new version in MLflow\n",
    "# This creates version 2 while preserving version 1 for comparison\n",
    "mlflow.genai.register_prompt(\n",
    "    name=\"qa_prompt\",\n",
    "    template=PROMPT_V2,\n",
    "    commit_message=\"Enhanced prompt with formatting instructions for conciseness and professionalism\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt V2 registered successfully!\")\n",
    "print(\"\\nğŸ“‹ Prompt V2 structure:\")\n",
    "for i, msg in enumerate(PROMPT_V2):\n",
    "    print(f\"  {i+1}. {msg['role'].title()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\nğŸ”„ Version loading options:\")\n",
    "print(\"  - prompts:/qa_prompt@latest (always gets the latest version)\")\n",
    "print(\"  - prompts:/qa_prompt/1 (load specific version 1)\")\n",
    "print(\"  - prompts:/qa_prompt/2 (load specific version 2)\")\n",
    "\n",
    "print(\"\\nğŸ¯ Key improvements in V2:\")\n",
    "print(\"  - Added length constraint (3 sentences max)\")\n",
    "print(\"  - Specified professional tone requirement\")\n",
    "print(\"  - Enhanced clarity and directness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43931e4",
   "metadata": {},
   "source": [
    "## ğŸ§ª Step 3: Create Evaluation Dataset\n",
    "\n",
    "A robust evaluation dataset is crucial for measuring prompt performance. We'll create comprehensive test cases covering diverse topics and difficulty levels to ensure our prompts work well across different scenarios.\n",
    "\n",
    "### Dataset Structure\n",
    "\n",
    "Each test case in our evaluation dataset includes:\n",
    "- **inputs**: The question to ask the LLM\n",
    "- **expectations**: Expected key concepts that should be mentioned in the response\n",
    "- **tags**: Metadata for filtering, analysis, and targeted improvements\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "Our dataset covers:\n",
    "- **Multiple domains**: Weather, technology, medicine, biology, environment\n",
    "- **Difficulty levels**: Basic, intermediate, and advanced questions\n",
    "- **Key concept validation**: Ensuring responses cover expected topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1170c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a comprehensive evaluation dataset with diverse test cases\n",
    "# Each entry contains inputs (the question), expectations (key concepts), and metadata tags\n",
    "eval_dataset = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What causes rain?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"evaporation\", \"condensation\", \"precipitation\", \"water cycle\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"weather\", \"difficulty\": \"basic\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Explain the difference between AI and ML\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"artificial intelligence\", \"machine learning\", \"subset\", \"algorithms\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"technology\", \"difficulty\": \"intermediate\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"How do vaccines work?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"immune system\", \"antibodies\", \"protection\", \"antigens\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"medicine\", \"difficulty\": \"intermediate\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is photosynthesis?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"sunlight\", \"chlorophyll\", \"carbon dioxide\", \"oxygen\", \"glucose\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"biology\", \"difficulty\": \"basic\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Describe quantum computing\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"quantum\", \"qubits\", \"superposition\", \"entanglement\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"technology\", \"difficulty\": \"advanced\"},\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is climate change?\"},\n",
    "        \"expectations\": {\n",
    "            \"key_concepts\": [\"greenhouse gases\", \"global warming\", \"carbon emissions\", \"temperature\"]\n",
    "        },\n",
    "        \"tags\": {\"topic\": \"environment\", \"difficulty\": \"intermediate\"},\n",
    "    },\n",
    "]\n",
    "\n",
    "# Display dataset statistics and composition\n",
    "print(f\"âœ… Created evaluation dataset with {len(eval_dataset)} test cases\")\n",
    "print(\"\\nğŸ“Š Dataset composition:\")\n",
    "\n",
    "# Calculate topic distribution\n",
    "topic_counts = {}\n",
    "difficulty_counts = {}\n",
    "\n",
    "for item in eval_dataset:\n",
    "    topic = item[\"tags\"][\"topic\"]\n",
    "    difficulty = item[\"tags\"][\"difficulty\"]\n",
    "    \n",
    "    topic_counts[topic] = topic_counts.get(topic, 0) + 1\n",
    "    difficulty_counts[difficulty] = difficulty_counts.get(difficulty, 0) + 1\n",
    "\n",
    "print(\"\\nğŸŒ Topics covered:\")\n",
    "for topic, count in topic_counts.items():\n",
    "    print(f\"  - {topic}: {count} cases\")\n",
    "\n",
    "print(\"\\nğŸ“ˆ Difficulty distribution:\")\n",
    "for difficulty, count in difficulty_counts.items():\n",
    "    print(f\"  - {difficulty}: {count} cases\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ Total key concepts to evaluate: {sum(len(item['expectations']['key_concepts']) for item in eval_dataset)}\")\n",
    "print(\"ğŸ¯ This diverse dataset will help us measure prompt performance across different domains and complexity levels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fbfbf8",
   "metadata": {},
   "source": [
    "## ğŸ”® Step 4: Create Prediction Function with Groq API\n",
    "\n",
    "The prediction function is the bridge between our prompt templates and the LLM. This function demonstrates how to integrate Groq API with MLflow's prompt registry for efficient, cost-effective inference.\n",
    "\n",
    "### Function Architecture\n",
    "\n",
    "Our prediction function performs these key operations:\n",
    "1. **Load Prompt Template**: Retrieves the latest version from MLflow registry\n",
    "2. **Format Template**: Fills in template variables with actual questions\n",
    "3. **Convert Format**: Transforms MLflow format to Groq API format\n",
    "4. **Generate Response**: Calls Groq API with optimized parameters\n",
    "5. **Return Result**: Provides response for evaluation\n",
    "\n",
    "### Key Benefits of Groq Integration\n",
    "\n",
    "- **âš¡ Speed**: Groq's optimized inference delivers responses in milliseconds\n",
    "- **ğŸ’° Cost-Effective**: Significantly cheaper than OpenAI API calls\n",
    "- **ğŸ”„ Consistency**: Reliable API with high uptime\n",
    "- **ğŸ“Š Tracking**: Full integration with MLflow tracing\n",
    "\n",
    "**Important**: The function parameter name must match the key in our dataset's `inputs` field (`question`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def predict_fn(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Prediction function that uses Groq API to generate responses.\n",
    "    \n",
    "    This function integrates MLflow prompt registry with Groq API for efficient\n",
    "    LLM inference. It automatically loads the latest prompt version and handles\n",
    "    format conversion between MLflow and Groq.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask (must match dataset inputs key)\n",
    "        \n",
    "    Returns:\n",
    "        str: The generated response from Groq API\n",
    "        \n",
    "    Raises:\n",
    "        Exception: If API call fails or prompt loading errors occur\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the latest prompt template from MLflow registry\n",
    "        # Using @latest syntax to always get the most recent version\n",
    "        prompt = mlflow.genai.load_prompt(\"prompts:/qa_prompt@latest\")\n",
    "        \n",
    "        # Format the prompt template with the actual question\n",
    "        # This replaces {{question}} with the real question from our dataset\n",
    "        rendered_prompt = prompt.format(question=question)\n",
    "        \n",
    "        # Convert MLflow prompt format to Groq API format\n",
    "        # Groq expects a list of message dictionaries with 'role' and 'content'\n",
    "        groq_messages = []\n",
    "        for msg in rendered_prompt:\n",
    "            groq_messages.append({\n",
    "                \"role\": msg[\"role\"],\n",
    "                \"content\": msg[\"content\"]\n",
    "            })\n",
    "        \n",
    "        # Call Groq API to generate response\n",
    "        # Using llama-3.3-70b-versatile for high-quality, fast inference\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",  # Fast, capable model\n",
    "            messages=groq_messages,\n",
    "            temperature=0.7,      # Balanced creativity vs consistency\n",
    "            max_tokens=500,       # Reasonable response length\n",
    "            top_p=1,             # Use full probability distribution\n",
    "            stream=False,        # Get complete response\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in prediction function: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test the prediction function with a sample question\n",
    "print(\"ğŸ§ª Testing prediction function with sample question...\")\n",
    "test_question = \"What causes rain?\"\n",
    "test_response = predict_fn(test_question)\n",
    "print(f\"Question: {test_question}\")\n",
    "print(f\"Response: {test_response}\")\n",
    "print(\"âœ… Prediction function working correctly!\")\n",
    "print(\"ğŸš€ Ready to run full evaluation on all test cases.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4a237a",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 5: Define Evaluation Scorers\n",
    "\n",
    "Evaluation scorers are the metrics that measure how well our prompts perform. We'll create both LLM-based and heuristic scorers to comprehensively assess response quality across multiple dimensions.\n",
    "\n",
    "### ğŸ§  Custom LLM Scorers Using Groq API\n",
    "\n",
    "These scorers use Groq models to evaluate qualitative aspects of responses. We've created custom scorers because MLflow's built-in Guidelines scorers are hardcoded to require OpenAI API keys.\n",
    "\n",
    "#### Scorer Categories\n",
    "\n",
    "Our custom LLM scorers evaluate:\n",
    "- **Conciseness**: Brevity and directness of responses\n",
    "- **Professionalism**: Tone and formality appropriateness\n",
    "- **Accuracy**: Factual correctness and reliability\n",
    "- **Helpfulness**: Usefulness and relevance to the question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54187be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom LLM-based scorers using Groq API\n",
    "# These scorers use Groq models to evaluate qualitative aspects of responses\n",
    "# Each scorer returns a Feedback object with a score (0-1) and rationale\n",
    "\n",
    "@scorer\n",
    "def is_concise(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate if the response is concise and to the point.\n",
    "    \n",
    "    This scorer assesses brevity and directness, penalizing unnecessary details\n",
    "    while rewarding clear, focused responses that get to the point quickly.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        evaluation_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator. Rate responses on a scale of 0-1 where 1 is perfect.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Evaluate this response for conciseness (brevity and directness):\n",
    "\n",
    "Response: \"{outputs}\"\n",
    "\n",
    "Guidelines: The response should be concise and to the point. Avoid unnecessary details.\n",
    "Score based on how well the response balances completeness with brevity.\n",
    "\n",
    "Provide your evaluation in this exact JSON format:\n",
    "{{\"score\": 0.85, \"rationale\": \"Brief explanation of your score\"}}\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=evaluation_prompt,\n",
    "            temperature=0.1,  # Low temperature for consistent evaluation\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        # Parse the JSON response from the LLM evaluator\n",
    "        import json\n",
    "        try:\n",
    "            eval_result = json.loads(result)\n",
    "            return Feedback(value=eval_result[\"score\"], rationale=eval_result[\"rationale\"])\n",
    "        except:\n",
    "            # Fallback if JSON parsing fails\n",
    "            return Feedback(value=0.5, rationale=f\"Could not parse evaluation: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return Feedback(value=0.0, rationale=f\"Evaluation error: {str(e)}\")\n",
    "\n",
    "@scorer\n",
    "def is_professional(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate if the response maintains professional tone.\n",
    "    \n",
    "    This scorer assesses whether the response uses appropriate language,\n",
    "    maintains formality, and presents information in a professional manner.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        evaluation_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator. Rate responses on a scale of 0-1 where 1 is perfect.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Evaluate this response for professional tone:\n",
    "\n",
    "Response: \"{outputs}\"\n",
    "\n",
    "Guidelines: The response should be written in a professional, clear, and appropriate tone.\n",
    "Consider language choice, formality level, and overall presentation.\n",
    "\n",
    "Provide your evaluation in this exact JSON format:\n",
    "{{\"score\": 0.85, \"rationale\": \"Brief explanation of your score\"}}\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=evaluation_prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        import json\n",
    "        try:\n",
    "            eval_result = json.loads(result)\n",
    "            return Feedback(value=eval_result[\"score\"], rationale=eval_result[\"rationale\"])\n",
    "        except:\n",
    "            return Feedback(value=0.5, rationale=f\"Could not parse evaluation: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return Feedback(value=0.0, rationale=f\"Evaluation error: {str(e)}\")\n",
    "\n",
    "@scorer\n",
    "def is_accurate(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate if the response is factually accurate.\n",
    "    \n",
    "    This scorer assesses the factual correctness of the response based on\n",
    "    established knowledge and verifiable information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        evaluation_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator. Rate responses on a scale of 0-1 where 1 is perfect.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Evaluate this response for factual accuracy:\n",
    "\n",
    "Response: \"{outputs}\"\n",
    "\n",
    "Guidelines: The response should be factually accurate and based on established knowledge.\n",
    "Consider whether the information presented is correct, verifiable, and reliable.\n",
    "\n",
    "Provide your evaluation in this exact JSON format:\n",
    "{{\"score\": 0.85, \"rationale\": \"Brief explanation of your score\"}}\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=evaluation_prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        import json\n",
    "        try:\n",
    "            eval_result = json.loads(result)\n",
    "            return Feedback(value=eval_result[\"score\"], rationale=eval_result[\"rationale\"])\n",
    "        except:\n",
    "            return Feedback(value=0.5, rationale=f\"Could not parse evaluation: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return Feedback(value=0.0, rationale=f\"Evaluation error: {str(e)}\")\n",
    "\n",
    "@scorer\n",
    "def is_helpful(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate if the response is helpful and addresses the question.\n",
    "    \n",
    "    This scorer assesses whether the response is useful, informative,\n",
    "    and directly addresses what the user is asking for.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        evaluation_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert evaluator. Rate responses on a scale of 0-1 where 1 is perfect.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"\"\"\n",
    "Evaluate this response for helpfulness:\n",
    "\n",
    "Response: \"{outputs}\"\n",
    "\n",
    "Guidelines: The response should be helpful, informative, and directly address the user's question.\n",
    "Consider relevance, completeness, and practical value.\n",
    "\n",
    "Provide your evaluation in this exact JSON format:\n",
    "{{\"score\": 0.85, \"rationale\": \"Brief explanation of your score\"}}\n",
    "\"\"\"}\n",
    "        ]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"llama-3.3-70b-versatile\",\n",
    "            messages=evaluation_prompt,\n",
    "            temperature=0.1,\n",
    "            max_tokens=200,\n",
    "        )\n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        import json\n",
    "        try:\n",
    "            eval_result = json.loads(result)\n",
    "            return Feedback(value=eval_result[\"score\"], rationale=eval_result[\"rationale\"])\n",
    "        except:\n",
    "            return Feedback(value=0.5, rationale=f\"Could not parse evaluation: {result}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        return Feedback(value=0.0, rationale=f\"Evaluation error: {str(e)}\")\n",
    "\n",
    "# Display summary of custom LLM scorers\n",
    "print(\"âœ… Custom Groq-based LLM scorers defined:\")\n",
    "print(\"  - is_concise: Evaluates brevity and directness using Groq\")\n",
    "print(\"  - is_professional: Evaluates tone and professionalism using Groq\") \n",
    "print(\"  - is_accurate: Evaluates factual correctness using Groq\")\n",
    "print(\"  - is_helpful: Evaluates usefulness and relevance using Groq\")\n",
    "print(\"\\nğŸ’¡ These scorers use Groq API instead of OpenAI, providing:\")\n",
    "print(\"  â€¢ Faster evaluation with lower latency\")\n",
    "print(\"  â€¢ Reduced costs compared to OpenAI API\")\n",
    "print(\"  â€¢ Consistent performance and reliability\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dcd847",
   "metadata": {},
   "source": [
    "### ğŸ”§ Alternative: Using Guidelines with Custom Model (Optional)\n",
    "\n",
    "If you want to try using MLflow's built-in Guidelines with a different model, you can attempt this approach. However, this may not work with all MLflow versions as Guidelines might be hardcoded to OpenAI.\n",
    "\n",
    "**Note**: This section demonstrates why custom scorers are necessary when working with non-OpenAI APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95dff8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach: Try to use Guidelines with custom model\n",
    "# This demonstrates why custom scorers are necessary for non-OpenAI APIs\n",
    "\n",
    "try:\n",
    "    # Attempt to create Guidelines with a custom model\n",
    "    # Note: This will likely fail as Guidelines are hardcoded to OpenAI\n",
    "    from mlflow.genai.scorers import Guidelines\n",
    "    \n",
    "    # Try to specify a custom model (this parameter may not exist)\n",
    "    is_concise_alt = Guidelines(\n",
    "        name=\"is_concise_alt\", \n",
    "        guidelines=\"The response should be concise and to the point. Avoid unnecessary details.\",\n",
    "        model=\"llama-3.3-70b-versatile\"  # This parameter may not be supported\n",
    "    )\n",
    "    print(\"âœ… Alternative Guidelines approach worked!\")\n",
    "    print(\"ğŸ’¡ If this works, you could use Guidelines instead of custom scorers\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Alternative Guidelines approach failed: {e}\")\n",
    "    print(\"ğŸ’¡ This confirms that Guidelines scorers are hardcoded to OpenAI\")\n",
    "    print(\"âœ… Our custom Groq-based scorers are the correct solution!\")\n",
    "    print(\"\\nğŸ” Why custom scorers are better:\")\n",
    "    print(\"  â€¢ Full control over evaluation prompts\")\n",
    "    print(\"  â€¢ Support for any LLM provider\")\n",
    "    print(\"  â€¢ Customizable scoring logic\")\n",
    "    print(\"  â€¢ Better error handling\")\n",
    "\n",
    "# Clean up any failed attempts\n",
    "try:\n",
    "    del is_concise_alt\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262cc79a",
   "metadata": {},
   "source": [
    "## ğŸ’¡ Solution Summary: Using Groq Instead of OpenAI for LLM Scorers\n",
    "\n",
    "### ğŸš¨ The Problem\n",
    "MLflow's built-in `Guidelines` scorers are hardcoded to use OpenAI API, which requires an `OPENAI_API_KEY` environment variable. This creates a dependency on OpenAI's API and pricing structure.\n",
    "\n",
    "### âœ… The Solution\n",
    "We've created **custom LLM scorers** that use Groq API instead. These scorers provide:\n",
    "\n",
    "1. **Fast Inference**: Use Groq's optimized infrastructure with `llama-3.3-70b-versatile` model\n",
    "2. **High Quality**: Maintain the same evaluation quality as OpenAI-based scorers\n",
    "3. **Structured Output**: Provide consistent JSON responses for reliable scoring\n",
    "4. **Robust Error Handling**: Include fallback mechanisms for evaluation failures\n",
    "5. **Cost Efficiency**: Significantly lower costs than OpenAI API calls\n",
    "\n",
    "### ğŸ¯ Benefits of Our Custom Approach\n",
    "\n",
    "| Feature | Custom Groq Scorers | MLflow Guidelines |\n",
    "|---------|-------------------|-------------------|\n",
    "| **API Provider** | âœ… Groq (flexible) | âŒ OpenAI only |\n",
    "| **Speed** | âœ… Ultra-fast | âš ï¸ Standard |\n",
    "| **Cost** | âœ… Very low | âŒ Higher |\n",
    "| **Customization** | âœ… Full control | âŒ Limited |\n",
    "| **Dependencies** | âœ… Minimal | âŒ OpenAI required |\n",
    "\n",
    "### ğŸ› ï¸ Implementation Advantages\n",
    "\n",
    "- **ğŸ”§ No OpenAI Dependency**: Works entirely with Groq ecosystem\n",
    "- **âš¡ Faster Inference**: Groq's optimized infrastructure delivers sub-second responses\n",
    "- **ğŸ’° Lower Costs**: Groq's competitive pricing reduces evaluation expenses\n",
    "- **ğŸ›ï¸ Full Customization**: Easy to modify evaluation prompts and models\n",
    "- **ğŸ” Better Control**: Fine-tune scoring logic for specific use cases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0637afd3",
   "metadata": {},
   "source": [
    "### ğŸ” Custom Heuristic Scorers\n",
    "\n",
    "These scorers use rule-based logic to evaluate specific aspects like concept coverage and response length. They provide fast, deterministic evaluations that complement our LLM-based scorers.\n",
    "\n",
    "#### Scorer Types\n",
    "\n",
    "Our heuristic scorers evaluate:\n",
    "- **Concept Coverage**: Percentage of expected key concepts mentioned\n",
    "- **Response Length**: Appropriate length based on specified criteria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcaefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom heuristic scorer for concept coverage\n",
    "# This scorer evaluates how many expected key concepts are mentioned in the response\n",
    "@scorer\n",
    "def concept_coverage(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate the coverage of key concepts in the response.\n",
    "    \n",
    "    This scorer performs case-insensitive matching to find expected key concepts\n",
    "    in the generated response and calculates a coverage percentage score.\n",
    "    \n",
    "    Args:\n",
    "        outputs (str): The generated response text\n",
    "        expectations (dict): Expected concepts and other criteria\n",
    "            - key_concepts: List of concepts that should be mentioned\n",
    "        \n",
    "    Returns:\n",
    "        Feedback: Score between 0-1 and detailed rationale\n",
    "    \"\"\"\n",
    "    # Extract expected key concepts from the expectations\n",
    "    concepts = set(expectations.get(\"key_concepts\", []))\n",
    "    \n",
    "    if not concepts:\n",
    "        return Feedback(\n",
    "            value=1.0,\n",
    "            rationale=\"No key concepts specified to evaluate - scoring as perfect\"\n",
    "        )\n",
    "    \n",
    "    # Convert response to lowercase for case-insensitive matching\n",
    "    response_lower = outputs.lower()\n",
    "    \n",
    "    # Find which concepts are mentioned in the response\n",
    "    included = set()\n",
    "    for concept in concepts:\n",
    "        if concept.lower() in response_lower:\n",
    "            included.add(concept)\n",
    "    \n",
    "    # Calculate coverage score as a percentage (0-1)\n",
    "    coverage_score = len(included) / len(concepts)\n",
    "    \n",
    "    # Generate detailed rationale for transparency\n",
    "    missing_concepts = concepts - included\n",
    "    rationale = f\"Coverage: {len(included)}/{len(concepts)} concepts included\"\n",
    "    if included:\n",
    "        rationale += f\" - Found: {list(included)}\"\n",
    "    if missing_concepts:\n",
    "        rationale += f\" - Missing: {list(missing_concepts)}\"\n",
    "    \n",
    "    return Feedback(\n",
    "        value=coverage_score,\n",
    "        rationale=rationale\n",
    "    )\n",
    "\n",
    "# Custom scorer for response length (conciseness check)\n",
    "@scorer  \n",
    "def response_length_check(outputs: str, expectations: dict) -> Feedback:\n",
    "    \"\"\"\n",
    "    Evaluate if the response length is appropriate (not too long or too short).\n",
    "    \n",
    "    This scorer checks if the response meets length requirements, penalizing\n",
    "    responses that are too short (lack detail) or too long (verbose).\n",
    "    \n",
    "    Args:\n",
    "        outputs (str): The generated response text\n",
    "        expectations (dict): Expected criteria\n",
    "            - max_length: Maximum acceptable word count (default: 100)\n",
    "        \n",
    "    Returns:\n",
    "        Feedback: Score between 0-1 and rationale\n",
    "    \"\"\"\n",
    "    word_count = len(outputs.split())\n",
    "    \n",
    "    # Define ideal length range (adjustable based on use case)\n",
    "    min_words = 10  # Minimum for substantive response\n",
    "    max_words = expectations.get(\"max_length\", 100)  # Maximum to avoid verbosity\n",
    "    \n",
    "    if word_count < min_words:\n",
    "        # Score decreases linearly for responses that are too short\n",
    "        score = word_count / min_words\n",
    "        rationale = f\"Response too short ({word_count} words, minimum {min_words} required)\"\n",
    "    elif word_count > max_words:\n",
    "        # Score decreases as response gets longer than maximum\n",
    "        score = max_words / word_count\n",
    "        rationale = f\"Response too long ({word_count} words, maximum {max_words} recommended)\"\n",
    "    else:\n",
    "        # Perfect score for responses within the ideal range\n",
    "        score = 1.0\n",
    "        rationale = f\"Response length optimal ({word_count} words within {min_words}-{max_words} range)\"\n",
    "    \n",
    "    return Feedback(\n",
    "        value=score,\n",
    "        rationale=rationale\n",
    "    )\n",
    "\n",
    "# Display summary of heuristic scorers\n",
    "print(\"âœ… Custom heuristic scorers defined:\")\n",
    "print(\"  - concept_coverage: Evaluates coverage of expected key concepts\")\n",
    "print(\"  - response_length_check: Evaluates appropriate response length\")\n",
    "print(\"\\nğŸ’¡ Heuristic scorers provide:\")\n",
    "print(\"  â€¢ Fast, deterministic evaluation\")\n",
    "print(\"  â€¢ Transparent scoring logic\")\n",
    "print(\"  â€¢ Consistent results across runs\")\n",
    "print(\"  â€¢ Complement to LLM-based scorers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fed418",
   "metadata": {},
   "source": [
    "## ğŸš€ Step 6: Run Evaluation\n",
    "\n",
    "Now we'll run the comprehensive evaluation using our prediction function and all scorers. This will test our current prompt (V2) against all test cases and provide detailed performance metrics.\n",
    "\n",
    "### Evaluation Process\n",
    "\n",
    "The evaluation will:\n",
    "1. **Load Current Prompt**: Use the latest version from MLflow registry\n",
    "2. **Process Test Cases**: Run each question through our prediction function\n",
    "3. **Apply All Scorers**: Evaluate responses using both LLM-based and heuristic scorers\n",
    "4. **Track Results**: Log everything to MLflow for analysis and comparison\n",
    "5. **Generate Metrics**: Provide comprehensive performance scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884c65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comprehensive evaluation with all our scorers\n",
    "print(\"ğŸš€ Starting comprehensive evaluation...\")\n",
    "print(\"This will evaluate our current prompt (V2) against all test cases.\")\n",
    "print(\"The evaluation may take a few minutes depending on the number of test cases.\\n\")\n",
    "\n",
    "# Combine all scorers into a single list for comprehensive evaluation\n",
    "all_scorers = [\n",
    "    is_concise,           # LLM-based: evaluates brevity and directness\n",
    "    is_professional,      # LLM-based: evaluates tone and professionalism\n",
    "    is_accurate,          # LLM-based: evaluates factual correctness\n",
    "    is_helpful,           # LLM-based: evaluates usefulness and relevance\n",
    "    concept_coverage,     # Heuristic: evaluates coverage of key concepts\n",
    "    response_length_check # Heuristic: evaluates appropriate response length\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“Š Evaluation configuration:\")\n",
    "print(f\"  â€¢ Test cases: {len(eval_dataset)}\")\n",
    "print(f\"  â€¢ Scorers: {len(all_scorers)} (4 LLM-based + 2 heuristic)\")\n",
    "print(f\"  â€¢ Prompt version: Latest from registry\")\n",
    "print(f\"  â€¢ Model: llama-3.3-70b-versatile via Groq API\")\n",
    "print(\"\\nğŸ”„ Starting evaluation process...\")\n",
    "\n",
    "# Run the evaluation using MLflow's GenAI evaluation framework\n",
    "# This will create a new run and log all results automatically\n",
    "evaluation_results = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,\n",
    "    scorers=all_scorers,\n",
    ")\n",
    "\n",
    "print(\"âœ… Evaluation completed successfully!\")\n",
    "print(f\"ğŸ“Š Results logged to MLflow run: {evaluation_results.run_id}\")\n",
    "print(\"\\nğŸ“ˆ To view detailed results:\")\n",
    "print(\"1. Open MLflow UI: http://localhost:5000\")\n",
    "print(\"2. Navigate to the latest experiment run\")\n",
    "print(\"3. Click on 'Evaluation Results' to see detailed scores\")\n",
    "print(\"4. Click on individual test cases to see traces and rationales\")\n",
    "print(\"5. Compare scores across different evaluation metrics\")\n",
    "print(\"\\nğŸ¯ Next: We'll create a third prompt version to demonstrate iterative improvement!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6c70a",
   "metadata": {},
   "source": [
    "## ğŸ”„ Step 7: Iterative Prompt Improvement\n",
    "\n",
    "Now we'll create a third version of our prompt to demonstrate the iterative improvement process. This version will be even more specific about formatting requirements, and we'll compare the results to show how prompt engineering can systematically improve performance.\n",
    "\n",
    "### Improvement Strategy\n",
    "\n",
    "Our V3 prompt will focus on:\n",
    "- **Precise Length Requirements**: \"exactly 2-3 sentences\" for consistency\n",
    "- **Enhanced Clarity**: More specific instructions for response structure\n",
    "- **Professional Standards**: Reinforced tone and quality expectations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9674a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an even more refined version of our prompt\n",
    "# This version adds precise length requirements and enhanced clarity instructions\n",
    "PROMPT_V3 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a knowledgeable assistant. Provide clear, concise answers in exactly 2-3 sentences. Focus on the most important information and use professional language.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Question: {{question}}\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Register the new version in MLflow prompt registry\n",
    "# This creates version 3 while preserving versions 1 and 2 for comparison\n",
    "mlflow.genai.register_prompt(\n",
    "    name=\"qa_prompt\",\n",
    "    template=PROMPT_V3,\n",
    "    commit_message=\"V3: More specific length and clarity requirements for improved consistency\",\n",
    ")\n",
    "\n",
    "print(\"âœ… Prompt V3 registered successfully!\")\n",
    "print(\"\\nğŸ“‹ Prompt V3 structure:\")\n",
    "for i, msg in enumerate(PROMPT_V3):\n",
    "    print(f\"  {i+1}. {msg['role'].title()}: {msg['content']}\")\n",
    "\n",
    "print(\"\\nğŸ¯ Key improvements in V3:\")\n",
    "print(\"  - Precise length constraint: 'exactly 2-3 sentences'\")\n",
    "print(\"  - Enhanced clarity: 'Focus on the most important information'\")\n",
    "print(\"  - Reinforced professionalism: 'knowledgeable assistant'\")\n",
    "print(\"  - Structured approach: Clear, concise, professional\")\n",
    "\n",
    "print(\"\\nğŸ”„ Running evaluation with V3 prompt...\")\n",
    "\n",
    "# Run evaluation with the new prompt (V3)\n",
    "# The prediction function will automatically use the latest prompt version\n",
    "evaluation_results_v3 = mlflow.genai.evaluate(\n",
    "    data=eval_dataset,\n",
    "    predict_fn=predict_fn,  # Automatically uses latest prompt (V3)\n",
    "    scorers=all_scorers,\n",
    ")\n",
    "\n",
    "print(\"âœ… V3 evaluation completed successfully!\")\n",
    "print(f\"ğŸ“Š V3 Results logged to MLflow run: {evaluation_results_v3.run_id}\")\n",
    "print(\"\\nğŸ” Now you can compare V2 and V3 results in the MLflow UI:\")\n",
    "print(\"1. Go to the experiment page in MLflow UI\")\n",
    "print(\"2. Select the V2 and V3 runs to compare\")\n",
    "print(\"3. View the comparison dashboard to see improvements/degradations\")\n",
    "print(\"4. Analyze which metrics improved and which may have declined\")\n",
    "print(\"\\nğŸ’¡ This demonstrates the iterative prompt improvement workflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13731a45",
   "metadata": {},
   "source": [
    "## ğŸ› ï¸ Step 8: Advanced Features and Tips\n",
    "\n",
    "### ğŸ“š Loading Specific Prompt Versions\n",
    "\n",
    "One of the key benefits of MLflow's prompt registry is the ability to load specific versions for testing, comparison, or rollback scenarios. This enables precise control over which prompt version is used in different environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b258a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load specific prompt versions for testing and comparison\n",
    "print(\"ğŸ“‹ Loading available prompt versions:\")\n",
    "\n",
    "# Load different versions using MLflow's prompt registry\n",
    "prompt_v1 = mlflow.genai.load_prompt(\"prompts:/qa_prompt/1\")\n",
    "prompt_v2 = mlflow.genai.load_prompt(\"prompts:/qa_prompt/2\") \n",
    "prompt_latest = mlflow.genai.load_prompt(\"prompts:/qa_prompt@latest\")\n",
    "\n",
    "print(\"âœ… Successfully loaded all prompt versions\")\n",
    "print(f\"  - V1: {len(prompt_v1)} messages\")\n",
    "print(f\"  - V2: {len(prompt_v2)} messages\") \n",
    "print(f\"  - Latest (V3): {len(prompt_latest)} messages\")\n",
    "\n",
    "print(\"\\nğŸ” Version comparison capabilities:\")\n",
    "print(\"  â€¢ Load specific versions for A/B testing\")\n",
    "print(\"  â€¢ Create environment-specific prompts (dev/staging/prod)\")\n",
    "print(\"  â€¢ Implement rollback strategies\")\n",
    "print(\"  â€¢ Compare prompt performance across versions\")\n",
    "\n",
    "# Create a custom prediction function for a specific version\n",
    "@mlflow.trace\n",
    "def predict_fn_v1(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Prediction function using prompt V1 specifically.\n",
    "    \n",
    "    This demonstrates how to create version-specific prediction functions\n",
    "    for comparing different prompt versions in the same evaluation.\n",
    "    \"\"\"\n",
    "    prompt = mlflow.genai.load_prompt(\"prompts:/qa_prompt/1\")\n",
    "    rendered_prompt = prompt.format(question=question)\n",
    "    \n",
    "    groq_messages = []\n",
    "    for msg in rendered_prompt:\n",
    "        groq_messages.append({\n",
    "            \"role\": msg[\"role\"],\n",
    "            \"content\": msg[\"content\"]\n",
    "        })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.3-70b-versatile\",\n",
    "        messages=groq_messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"\\nğŸ”„ Advanced comparison workflow:\")\n",
    "print(\"1. Create separate prediction functions for each version\")\n",
    "print(\"2. Run evaluations with different functions\")\n",
    "print(\"3. Compare results in MLflow UI side-by-side\")\n",
    "print(\"4. Identify which version performs best for specific metrics\")\n",
    "print(\"5. Implement the best-performing version in production\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Pro tip: You can also evaluate multiple versions simultaneously\")\n",
    "print(\"   by running separate evaluations and comparing the results!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fe33c0",
   "metadata": {},
   "source": [
    "## ğŸ‰ Summary and Next Steps\n",
    "\n",
    "### ğŸ† What We've Accomplished\n",
    "\n",
    "This comprehensive tutorial has demonstrated a complete prompt engineering workflow:\n",
    "\n",
    "1. **âœ… Setup & Configuration**: MLflow tracking and Groq API integration\n",
    "2. **âœ… Prompt Versioning**: Created and managed multiple prompt versions (V1, V2, V3)\n",
    "3. **âœ… Evaluation Dataset**: Built comprehensive test cases across diverse domains\n",
    "4. **âœ… LLM Integration**: Implemented prediction functions using Groq API\n",
    "5. **âœ… Custom Scorers**: Defined both LLM-based and heuristic evaluation metrics\n",
    "6. **âœ… Systematic Evaluation**: Ran evaluations and demonstrated iterative improvement\n",
    "\n",
    "### ğŸ¯ Key Benefits of This Approach\n",
    "\n",
    "| Benefit | Description |\n",
    "|---------|-------------|\n",
    "| **ğŸ”„ Systematic Iteration** | Track prompt changes and measure their impact on performance |\n",
    "| **ğŸ“Š Quantitative Evaluation** | Measure improvements with concrete, comparable metrics |\n",
    "| **ğŸ¯ Targeted Improvements** | Identify specific areas for enhancement based on data |\n",
    "| **ğŸ“ˆ Performance Tracking** | Monitor trends and performance over time |\n",
    "| **ğŸ” Detailed Analysis** | Deep dive into individual test cases and evaluation traces |\n",
    "| **ğŸ’° Cost Efficiency** | Use Groq's fast, affordable API instead of expensive alternatives |\n",
    "| **ğŸ›¡ï¸ Version Control** | Maintain prompt history and enable rollback capabilities |\n",
    "\n",
    "### ğŸš€ Next Steps for Further Development\n",
    "\n",
    "#### Immediate Improvements\n",
    "1. **ğŸ“Š Expand Evaluation Dataset**: Add more diverse questions across different domains\n",
    "2. **ğŸ¯ Domain-Specific Scorers**: Create specialized evaluation metrics for your use case\n",
    "3. **ğŸ¤– Model Experimentation**: Test different Groq models and parameters\n",
    "4. **ğŸ”„ A/B Testing Framework**: Implement systematic prompt variation testing\n",
    "\n",
    "#### Advanced Features\n",
    "1. **âš¡ Automated Pipelines**: Set up continuous evaluation and monitoring\n",
    "2. **ğŸ“ˆ Performance Dashboards**: Create real-time monitoring of prompt performance\n",
    "3. **ğŸ›ï¸ Hyperparameter Tuning**: Optimize temperature, max_tokens, and other parameters\n",
    "4. **ğŸ”— Multi-Model Evaluation**: Compare performance across different LLM providers\n",
    "\n",
    "#### Production Considerations\n",
    "1. **ğŸ­ Deployment Strategies**: Implement prompt versioning in production environments\n",
    "2. **ğŸ“Š Monitoring & Alerting**: Set up alerts for performance degradation\n",
    "3. **ğŸ”„ Rollback Mechanisms**: Implement automated rollback for poor-performing prompts\n",
    "4. **ğŸ“ Documentation**: Maintain comprehensive documentation of prompt evolution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba179a0",
   "metadata": {},
   "source": [
    "### ğŸ”§ Troubleshooting Tips\n",
    "\n",
    "**Common Issues and Solutions:**\n",
    "\n",
    "#### 1. **ğŸ”‘ Groq API Key Issues**\n",
    "```python\n",
    "# Make sure your API key is set correctly\n",
    "export GROQ_API_KEY=\"your-actual-api-key\"\n",
    "# Or set it in the notebook cell\n",
    "```\n",
    "\n",
    "**Symptoms**: Authentication errors, \"API key not configured\" messages\n",
    "**Solution**: Verify your API key is valid and properly set as an environment variable\n",
    "\n",
    "#### 2. **ğŸ”— MLflow Connection Issues**\n",
    "```bash\n",
    "# Start MLflow server if not running\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5000\n",
    "```\n",
    "\n",
    "**Symptoms**: Connection refused errors, \"Failed to log span\" warnings\n",
    "**Solution**: Ensure MLflow server is running on the specified port\n",
    "\n",
    "#### 3. **â±ï¸ Evaluation Taking Too Long**\n",
    "```python\n",
    "# Reduce dataset size for faster testing\n",
    "small_dataset = eval_dataset[:3]  # Use only first 3 test cases\n",
    "```\n",
    "\n",
    "**Symptoms**: Long evaluation times, timeout errors\n",
    "**Solution**: Start with smaller datasets for testing, then scale up\n",
    "\n",
    "#### 4. **ğŸ’¾ Memory Issues with Large Datasets**\n",
    "```python\n",
    "# Process evaluations in batches\n",
    "batch_size = 5\n",
    "for i in range(0, len(eval_dataset), batch_size):\n",
    "    batch = eval_dataset[i:i+batch_size]\n",
    "    mlflow.genai.evaluate(data=batch, predict_fn=predict_fn, scorers=all_scorers)\n",
    "```\n",
    "\n",
    "**Symptoms**: Memory errors, system slowdown\n",
    "**Solution**: Process evaluations in smaller batches\n",
    "\n",
    "#### 5. **ğŸ”„ Prompt Loading Issues**\n",
    "```python\n",
    "# Check if prompt exists before loading\n",
    "try:\n",
    "    prompt = mlflow.genai.load_prompt(\"prompts:/qa_prompt@latest\")\n",
    "except Exception as e:\n",
    "    print(f\"Prompt loading failed: {e}\")\n",
    "```\n",
    "\n",
    "**Symptoms**: \"Prompt not found\" errors\n",
    "**Solution**: Ensure prompts are registered before trying to load them\n",
    "\n",
    "### ğŸ“š Resources and Documentation\n",
    "\n",
    "| Resource | Description | URL |\n",
    "|----------|-------------|-----|\n",
    "| **MLflow GenAI Docs** | Official MLflow GenAI documentation | https://mlflow.org/docs/latest/genai/ |\n",
    "| **Groq API Docs** | Groq API reference and guides | https://console.groq.com/docs/ |\n",
    "| **Prompt Registry** | MLflow prompt registry documentation | https://mlflow.org/docs/latest/genai/prompt-registry/ |\n",
    "| **Evaluation Framework** | MLflow evaluation framework guide | https://mlflow.org/docs/latest/genai/eval-monitor/ |\n",
    "| **Groq Models** | Available models and capabilities | https://console.groq.com/docs/models |\n",
    "\n",
    "### ğŸ¯ Best Practices\n",
    "\n",
    "1. **ğŸ”’ Security**: Never commit API keys to version control\n",
    "2. **ğŸ“Š Monitoring**: Set up alerts for evaluation failures\n",
    "3. **ğŸ”„ Versioning**: Use semantic versioning for prompt changes\n",
    "4. **ğŸ“ Documentation**: Document prompt evolution and performance changes\n",
    "5. **ğŸ§ª Testing**: Always test with small datasets before full evaluation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
